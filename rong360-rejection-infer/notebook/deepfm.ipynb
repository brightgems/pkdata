{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拒绝推断\n",
    "> 使用tensorflow 建立回归模型， 并使用拒绝推断方法对无标签数据打分\n",
    "## 分配法\n",
    "此方法是依据每一个分数区间的好坏重新分配好坏客户，并将拒绝件分配到每一个分数区间中，以下是具体步骤：\n",
    "\n",
    "1） 已知好坏样本，建立初步模型；\n",
    "\n",
    "2） 使用初步模型对所有被拒绝件进行评分，并预测他们的预期违约率；\n",
    "\n",
    "3） 将已知的好坏样本依评分分数高低进行分组，计算各分组内实际违约率；\n",
    "\n",
    "4） 将被拒件依照3）步的分数进行分组，以各分组的实际违约率做为抽样比例，随机抽取该分组下的被拒件，并指定其为坏，其余为好；\n",
    "\n",
    "5） 将这些推论的有标签的数据新增加到原有的数据集中，重新建模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python27\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import multiprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../data/feature-selected/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = pd.read_pickle('../data/feature-selected/eval.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_dt</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f5</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f18</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f28</th>\n",
       "      <th>...</th>\n",
       "      <th>f6725</th>\n",
       "      <th>f6728</th>\n",
       "      <th>f6734</th>\n",
       "      <th>f6735</th>\n",
       "      <th>f6737</th>\n",
       "      <th>f6742</th>\n",
       "      <th>f6743</th>\n",
       "      <th>f6745</th>\n",
       "      <th>label</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>2018-03-07</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>442.180084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024681</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033447</td>\n",
       "      <td>...</td>\n",
       "      <td>27.620493</td>\n",
       "      <td>0.200</td>\n",
       "      <td>450.100006</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>20293.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.197765</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>2018-03-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>5.3333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14.480687</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6638.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.197485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.327245</td>\n",
       "      <td>...</td>\n",
       "      <td>11.838667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3893.979980</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>32251.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.341463</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.2727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.462925</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>36614.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>43.675888</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.096890</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.007577</td>\n",
       "      <td>...</td>\n",
       "      <td>8.279123</td>\n",
       "      <td>0.125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.253415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2940 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loan_dt    f1    f2          f5       f14      f15  f18       f20  \\\n",
       "id                                                                            \n",
       "100001 2018-03-07  57.0   0.0  442.180084       NaN   5.0000    0  0.024681   \n",
       "100002 2018-03-07   0.0   NaN    0.000000  0.000039   5.3333    0  0.000000   \n",
       "100003 2018-01-27   0.0   NaN   21.197485       NaN   3.5000    0  0.013158   \n",
       "100004 2018-03-29   0.0   NaN    0.000000       NaN   5.2727    0  0.006873   \n",
       "100005 2018-01-23   0.0  41.0   43.675888  0.000208  23.0000    0  0.096890   \n",
       "\n",
       "         f21       f28 ...       f6725  f6728        f6734     f6735    f6737  \\\n",
       "id                     ...                                                      \n",
       "100001   1.0  0.033447 ...   27.620493  0.200   450.100006  0.000851  20293.0   \n",
       "100002   NaN  0.000000 ...   14.480687  0.000          NaN  0.000000   6638.0   \n",
       "100003   NaN  0.327245 ...   11.838667  0.000  3893.979980  0.002193  32251.0   \n",
       "100004   NaN  0.000000 ...   24.462925  0.000          NaN  0.004009  36614.0   \n",
       "100005  99.0  0.007577 ...    8.279123  0.125          NaN  0.000000  16384.0   \n",
       "\n",
       "        f6742  f6743      f6745  label  tag  \n",
       "id                                           \n",
       "100001      0    0.0  49.197765      0    0  \n",
       "100002      0    0.0   0.000000      0    0  \n",
       "100003      0    0.0  27.341463      0    0  \n",
       "100004      0    0.0   0.000000      0    0  \n",
       "100005      0    0.0  15.253415      0    0  \n",
       "\n",
       "[5 rows x 2940 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid = pd.read_pickle('../data/feature-selected/valid.pkl')\n",
    "valid['label']=0\n",
    "valid['tag']=0\n",
    "valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======config=========\n",
    "\n",
    "SUB_DIR = \"./\"\n",
    "\n",
    "\n",
    "NUM_SPLITS = 3\n",
    "RANDOM_SEED = 233\n",
    "\n",
    "# types of columns of the dataset dataframe\n",
    "fdump = './categorical_features.pkl'\n",
    "CATEGORICAL_COLS = pkl.load(open(fdump,'rb'))\n",
    "\n",
    "NUMERIC_COLS = np.setdiff1d(list(train.columns),CATEGORICAL_COLS+['loan_dt','tag','label'])\n",
    "IGNORE_COLS = ['loan_dt','tag','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import os\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "\n",
    "def gini(actual, pred):\n",
    "    assert (len(actual) == len(pred))\n",
    "    #np.c_是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等，类似于pandas中的merge()。\n",
    "    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n",
    "    #np.arange创建等差数组，其实是生成index:0~n-1\n",
    "    #pred 列取负数：-1 * all[:, 1]\n",
    "    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]  #把pred列的分数由高到低排序\n",
    "    totalLosses = all[:, 0].sum()\n",
    "    giniSum = all[:, 0].cumsum().sum() / totalLosses\n",
    "\n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    "\n",
    "def gini_norm(actual, pred):\n",
    "    return gini(actual, pred) / gini(actual, actual)\n",
    "\n",
    "\n",
    "def custom_error(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    \n",
    "\n",
    "    return 'gini_norm',gini_norm(labels,preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========data_reader========\n",
    "import time\n",
    "#-------------------------------------------------\n",
    "# 计时装饰器\n",
    "#-------------------------------------------------\n",
    "def timeit(f):\n",
    "\n",
    "    def timed(*args, **kw):\n",
    "\n",
    "        ts = time.time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time.time()\n",
    "\n",
    "        print('func:%r took: %2.4f sec' % \\\n",
    "          (f.__name__, te-ts)\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    return timed\n",
    "\n",
    "class FeatureDictionary(object):\n",
    "    def __init__(self, trainfile=None, testfile=None,\n",
    "                 dfTrain=None, dfTest=None, numeric_cols=[], ignore_cols=[]):\n",
    "        assert not ((trainfile is None) and (dfTrain is None)), \"trainfile or dfTrain at least one is set\"\n",
    "        assert not ((trainfile is not None) and (dfTrain is not None)), \"only one can be set\"\n",
    "        assert not ((testfile is None) and (dfTest is None)), \"testfile or dfTest at least one is set\"\n",
    "        assert not ((testfile is not None) and (dfTest is not None)), \"only one can be set\"\n",
    "        self.trainfile = trainfile\n",
    "        self.testfile = testfile\n",
    "        self.dfTrain = dfTrain\n",
    "        self.dfTest = dfTest\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.ignore_cols = ignore_cols\n",
    "        self.gen_feat_dict()\n",
    "\n",
    "    @timeit\n",
    "    def gen_feat_dict(self):\n",
    "        if self.dfTrain is None:\n",
    "            dfTrain = pd.read_csv(self.trainfile)\n",
    "        else:\n",
    "            dfTrain = self.dfTrain\n",
    "        if self.dfTest is None:\n",
    "            dfTest = pd.read_csv(self.testfile)\n",
    "        else:\n",
    "            dfTest = self.dfTest\n",
    "        \n",
    "        df = pd.concat([dfTrain, dfTest]) #首尾相连\n",
    "        self.feat_dict = {}\n",
    "        tc = 0\n",
    "        for col in df.columns:\n",
    "            if col in self.ignore_cols:\n",
    "                continue\n",
    "            if col in self.numeric_cols:\n",
    "                # map to a single index\n",
    "                self.feat_dict[col] = tc\n",
    "                tc += 1\n",
    "            else:\n",
    "                us = df[col].unique()\n",
    "                self.feat_dict[col] = dict(zip(us, range(tc, len(us)+tc)))\n",
    "                tc += len(us)\n",
    "        self.feat_dim = tc\n",
    "\n",
    "\n",
    "class DataParser(object):\n",
    "    def __init__(self, feat_dict):\n",
    "        self.feat_dict = feat_dict\n",
    "\n",
    "    @timeit\n",
    "    def parse(self, infile=None, df=None, has_label=False):\n",
    "        assert not ((infile is None) and (df is None)), \"infile or df at least one is set\"\n",
    "        assert not ((infile is not None) and (df is not None)), \"only one can be set\"\n",
    "        if infile is None:\n",
    "            dfv = df.copy()\n",
    "        else:\n",
    "            dfv = pd.read_csv(infile)\n",
    "        if has_label:\n",
    "            y = dfv[\"label\"].values.tolist()\n",
    "        else:\n",
    "            ids = dfv.index.tolist()\n",
    "            \n",
    "        # dfi for feature index\n",
    "        # dfv for feature value which can be either binary (1/0) or float (e.g., 10.24)\n",
    "        fd = self.feat_dict\n",
    "        dfi = pd.DataFrame(np.tile(list(fd.feat_dict.values())[:len(fd.numeric_cols)],(df.shape[0],1)), columns=NUMERIC_COLS)\n",
    "        \n",
    "        for col in dfv.columns:\n",
    "            if col in self.feat_dict.ignore_cols:\n",
    "                dfv.drop(col, axis=1, inplace=True)\n",
    "                continue\n",
    "            \n",
    "            if col in self.feat_dict.numeric_cols:\n",
    "                pass\n",
    "            else:\n",
    "                dfi[col] = dfv[col].map(self.feat_dict.feat_dict[col])\n",
    "                dfv[col] = 1.\n",
    "\n",
    "        # list of list of feature indices of each sample in the dataset\n",
    "        Xi = dfi.values.tolist()\n",
    "        # list of list of feature values of each sample in the dataset\n",
    "        Xv = dfv.values.tolist()\n",
    "        if has_label:\n",
    "            return Xi, Xv, y\n",
    "        else:\n",
    "            return Xi, Xv, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2904"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NUMERIC_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数值型特征，normalize处理\n",
    "from sklearn.preprocessing  import MinMaxScaler\n",
    "from sklearn.preprocessing.imputation import Imputer\n",
    "\n",
    "train = train.fillna(0)\n",
    "tdata = train.drop(['label', 'tag', 'loan_dt'],axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(tdata)\n",
    "\n",
    "def read_data(data):\n",
    "    dfX = data.copy()\n",
    "    dfX = dfX.fillna(0)\n",
    "    if 'label' in dfX.columns:\n",
    "        y = dfX.label\n",
    "        dfX = dfX.drop('label', axis=1)\n",
    "    else:\n",
    "        y = None\n",
    "    X= scaler.transform(dfX)\n",
    "    return data,X,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "\n",
    "gini_scorer = make_scorer(gini_norm, greater_is_better=True, needs_proba=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def _load_data(train, eval):\n",
    "    train_data = pd.concat([ train[NUMERIC_COLS],train[CATEGORICAL_COLS+['label']]], axis=1)\n",
    "    test_data = pd.concat([ eval[NUMERIC_COLS],eval[CATEGORICAL_COLS]], axis=1)\n",
    "    dfTrain,X_train,y_train = read_data(train_data)\n",
    "    dfTest,X_test,_ = read_data(test_data)\n",
    "    ids_test = dfTest.index.tolist()\n",
    "    cat_features_indices = [i for i,c in enumerate(dfTrain.columns) if c in CATEGORICAL_COLS]\n",
    "\n",
    "    return dfTrain, dfTest, X_train, y_train, X_test, ids_test, cat_features_indices\n",
    "\n",
    "\n",
    "def _run_base_model_dfm(dfTrain, dfTest, folds, dfm_params):\n",
    "    fd = FeatureDictionary(dfTrain=dfTrain, dfTest=dfTest,\n",
    "                           numeric_cols=NUMERIC_COLS,\n",
    "                           ignore_cols=IGNORE_COLS)\n",
    "    data_parser = DataParser(feat_dict=fd)\n",
    "    Xi_train, Xv_train, y_train = data_parser.parse(df=dfTrain, has_label=True)\n",
    "    Xi_test, Xv_test, ids_test = data_parser.parse(df=dfTest)\n",
    "\n",
    "    dfm_params[\"feature_size\"] = fd.feat_dim\n",
    "    dfm_params[\"field_size\"] = len(Xi_train[0])\n",
    "\n",
    "    y_train_meta = np.zeros((dfTrain.shape[0], 1), dtype=float)\n",
    "    y_test_meta = np.zeros((dfTest.shape[0], 1), dtype=float)\n",
    "    _get = lambda x, l: [x[i] for i in l]\n",
    "    gini_results_cv = np.zeros(len(folds), dtype=float)\n",
    "    gini_results_epoch_train = np.zeros((len(folds), dfm_params[\"epoch\"]), dtype=float)\n",
    "    gini_results_epoch_valid = np.zeros((len(folds), dfm_params[\"epoch\"]), dtype=float)\n",
    "\n",
    "    for i, (train_idx, valid_idx) in enumerate(folds):\n",
    "        #valid_idx = valid_idx[:1024]\n",
    "        Xi_train_, Xv_train_, y_train_ = _get(Xi_train, train_idx), _get(Xv_train, train_idx), _get(y_train, train_idx)\n",
    "        Xi_valid_, Xv_valid_, y_valid_ = _get(Xi_train, valid_idx), _get(Xv_train, valid_idx), _get(y_train, valid_idx)\n",
    "\n",
    "        dfm = DeepFM(**dfm_params)\n",
    "        dfm.fit(Xi_train_, Xv_train_, y_train_, Xi_valid_, Xv_valid_, y_valid_)\n",
    "        #dfm.fit(Xi_train_, Xv_train_, y_train_, Xi_valid_, Xv_valid_, y_valid_,early_stopping=True)\n",
    "        y_train_meta[valid_idx,0]= dfm.predict(Xi_valid_, Xv_valid_)\n",
    "        y_test_meta[:,0] += dfm.predict(Xi_test, Xv_test)\n",
    "\n",
    "        gini_results_cv[i] = gini_norm(y_valid_, y_train_meta[valid_idx])\n",
    "        gini_results_epoch_train[i] = dfm.train_result\n",
    "        gini_results_epoch_valid[i] = dfm.valid_result\n",
    "\n",
    "    y_test_meta /= float(len(folds))\n",
    "\n",
    "    # save result\n",
    "    if dfm_params[\"use_fm\"] and dfm_params[\"use_deep\"]:\n",
    "        clf_str = \"DeepFM\"\n",
    "    elif dfm_params[\"use_fm\"]:\n",
    "        clf_str = \"FM\"\n",
    "    elif dfm_params[\"use_deep\"]:\n",
    "        clf_str = \"DNN\"\n",
    "    print(\"%s: %.5f (%.5f)\"%(clf_str, gini_results_cv.mean(), gini_results_cv.std()))\n",
    "    filename = \"%s_Mean%.5f_Std%.5f.csv\"%(clf_str, gini_results_cv.mean(), gini_results_cv.std())\n",
    "    _save_result(ids_test, y_test_meta, filename)\n",
    "\n",
    "    _plot_fig(gini_results_epoch_train, gini_results_epoch_valid, clf_str)\n",
    "\n",
    "    return y_train_meta, y_test_meta\n",
    "\n",
    "def _save_result(ids, y_pred, filename=\"dfm_res.csv\"):\n",
    "    pd.DataFrame({\"id\": ids, \"target\": y_pred.flatten()}).to_csv(\n",
    "    os.path.join(SUB_DIR, filename), index=False, float_format=\"%.5f\")\n",
    "\n",
    "\n",
    "\n",
    "def _plot_fig(train_results, valid_results, model_name):\n",
    "    colors = [\"red\", \"blue\", \"green\"]\n",
    "    xs = np.arange(1, train_results.shape[1]+1)\n",
    "    plt.figure()\n",
    "    legends = []\n",
    "    for i in range(train_results.shape[0]):\n",
    "        plt.plot(xs, train_results[i], color=colors[i], linestyle=\"solid\", marker=\"o\")\n",
    "        plt.plot(xs, valid_results[i], color=colors[i], linestyle=\"dashed\", marker=\"o\")\n",
    "        legends.append(\"train-%d\"%(i+1))\n",
    "        legends.append(\"valid-%d\"%(i+1))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Normalized Gini\")\n",
    "    plt.title(\"%s\"%model_name)\n",
    "    plt.legend(legends)\n",
    "    plt.savefig(\"./fig/%s.png\"%model_name)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(actual, pred):\n",
    "    dfm_res= []\n",
    "    for i in pred:\n",
    "        if i >0.5:\n",
    "            dfm_res.append(1)\n",
    "        else:\n",
    "            dfm_res.append(0)\n",
    "    print(\"pred: %d , actual: %d , total : %d\" % (sum(dfm_res),sum(actual),len(pred)))\n",
    "    \n",
    "    s = sklearn.metrics.accuracy_score(actual,dfm_res)\n",
    "    return s\n",
    "def gini_norm(actual, pred):\n",
    "    dfm_res= []\n",
    "    for i in pred:\n",
    "        if i >0.5:\n",
    "            dfm_res.append(1)\n",
    "        else:\n",
    "            dfm_res.append(0)\n",
    "    print(\"pred: %d , actual: %d , total : %d\" % (sum(dfm_res),sum(actual),len(pred)))\n",
    "    return gini(actual, pred) / gini(actual, actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load DeepFM\n",
    "\n",
    "\"\"\"\n",
    "Tensorflow implementation of DeepFM [1]\n",
    "Reference:\n",
    "[1] DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,\n",
    "    Huifeng Guo\u0003, Ruiming Tang, Yunming Yey, Zhenguo Li, Xiuqiang He.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "from yellowfin import YFOptimizer\n",
    "\n",
    "\n",
    "class DeepFM(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_size, field_size,\n",
    "                 embedding_size=8, dropout_fm=[1.0, 1.0],\n",
    "                 deep_layers=[32, 32], dropout_deep=[0.5, 0.5, 0.5],\n",
    "                 deep_layers_activation=tf.nn.relu,\n",
    "                 epoch=10, batch_size=256,\n",
    "                 learning_rate=0.001, optimizer_type=\"adam\",\n",
    "                 batch_norm=0, batch_norm_decay=0.995,\n",
    "                 verbose=False, random_seed=2016,\n",
    "                 use_fm=True, use_deep=True,\n",
    "                 loss_type=\"logloss\", eval_metric=roc_auc_score,\n",
    "                 l2_reg=0.0, greater_is_better=True):\n",
    "        assert (use_fm or use_deep)\n",
    "        assert loss_type in [\"logloss\", \"mse\"], \\\n",
    "            \"loss_type can be either 'logloss' for classification task or 'mse' for regression task\"\n",
    "\n",
    "        self.feature_size = feature_size        # denote as M, size of the feature dictionary\n",
    "        self.field_size = field_size            # denote as F, size of the feature fields\n",
    "        self.embedding_size = embedding_size    # denote as K, size of the feature embedding\n",
    "\n",
    "        self.dropout_fm = dropout_fm\n",
    "        self.deep_layers = deep_layers\n",
    "        self.dropout_deep = dropout_deep\n",
    "        self.deep_layers_activation = deep_layers_activation\n",
    "        self.use_fm = use_fm\n",
    "        self.use_deep = use_deep\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_type = optimizer_type\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_decay = batch_norm_decay\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "        self.loss_type = loss_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.greater_is_better = greater_is_better\n",
    "        self.train_result, self.valid_result = [], []\n",
    "\n",
    "        self._init_graph()\n",
    "\n",
    "\n",
    "    def _init_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "\n",
    "            self.feat_index = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                                                 name=\"feat_index\")  # None * F\n",
    "            self.feat_value = tf.placeholder(tf.float32, shape=[None, None],\n",
    "                                                 name=\"feat_value\")  # None * F\n",
    "            self.label = tf.placeholder(tf.float32, shape=[None, 1], name=\"label\")  # None * 1\n",
    "            self.dropout_keep_fm = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_fm\")\n",
    "            self.dropout_keep_deep = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_deep\")\n",
    "            self.train_phase = tf.placeholder(tf.bool, name=\"train_phase\")\n",
    "\n",
    "            self.weights = self._initialize_weights()\n",
    "\n",
    "            # model\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights[\"feature_embeddings\"],\n",
    "                                                             self.feat_index)  # None * F * K\n",
    "            feat_value = tf.reshape(self.feat_value, shape=[-1, self.field_size, 1])\n",
    "            self.embeddings = tf.multiply(self.embeddings, feat_value)\n",
    "\n",
    "            # ---------- first order term ----------\n",
    "            self.y_first_order = tf.nn.embedding_lookup(self.weights[\"feature_bias\"], self.feat_index) # None * F * 1\n",
    "            self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order, feat_value), 2)  # None * F\n",
    "            self.y_first_order = tf.nn.dropout(self.y_first_order, self.dropout_keep_fm[0]) # None * F\n",
    "\n",
    "            # ---------- second order term ---------------\n",
    "            # sum_square part\n",
    "            self.summed_features_emb = tf.reduce_sum(self.embeddings, 1)  # None * K\n",
    "            self.summed_features_emb_square = tf.square(self.summed_features_emb)  # None * K\n",
    "\n",
    "            # square_sum part\n",
    "            self.squared_features_emb = tf.square(self.embeddings)\n",
    "            self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, 1)  # None * K\n",
    "\n",
    "            # second order\n",
    "            self.y_second_order = 0.5 * tf.subtract(self.summed_features_emb_square, self.squared_sum_features_emb)  # None * K\n",
    "            self.y_second_order = tf.nn.dropout(self.y_second_order, self.dropout_keep_fm[1])  # None * K\n",
    "\n",
    "            # ---------- Deep component ----------\n",
    "            self.y_deep = tf.reshape(self.embeddings, shape=[-1, self.field_size * self.embedding_size]) # None * (F*K)\n",
    "            self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[0])\n",
    "            for i in range(0, len(self.deep_layers)):\n",
    "                self.y_deep = tf.add(tf.matmul(self.y_deep, self.weights[\"layer_%d\" %i]), self.weights[\"bias_%d\"%i]) # None * layer[i] * 1\n",
    "                if self.batch_norm:\n",
    "                    self.y_deep = self.batch_norm_layer(self.y_deep, train_phase=self.train_phase, scope_bn=\"bn_%d\" %i) # None * layer[i] * 1\n",
    "                self.y_deep = self.deep_layers_activation(self.y_deep)\n",
    "                self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[1+i]) # dropout at each Deep layer\n",
    "\n",
    "            # ---------- DeepFM ----------\n",
    "            if self.use_fm and self.use_deep:\n",
    "                concat_input = tf.concat([self.y_first_order, self.y_second_order, self.y_deep], axis=1)\n",
    "            elif self.use_fm:\n",
    "                concat_input = tf.concat([self.y_first_order, self.y_second_order], axis=1)\n",
    "            elif self.use_deep:\n",
    "                concat_input = self.y_deep\n",
    "            self.out = tf.add(tf.matmul(concat_input, self.weights[\"concat_projection\"]), self.weights[\"concat_bias\"])\n",
    "\n",
    "            # loss\n",
    "            if self.loss_type == \"logloss\":\n",
    "                self.out = tf.nn.sigmoid(self.out)\n",
    "                self.loss = tf.losses.log_loss(self.label, self.out)\n",
    "            elif self.loss_type == \"mse\":\n",
    "                self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))\n",
    "            # l2 regularization on weights\n",
    "            if self.l2_reg > 0:\n",
    "                self.loss += tf.contrib.layers.l2_regularizer(\n",
    "                    self.l2_reg)(self.weights[\"concat_projection\"])\n",
    "                if self.use_deep:\n",
    "                    for i in range(len(self.deep_layers)):\n",
    "                        self.loss += tf.contrib.layers.l2_regularizer(\n",
    "                            self.l2_reg)(self.weights[\"layer_%d\"%i])\n",
    "\n",
    "            # optimizer\n",
    "            if self.optimizer_type == \"adam\":\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,\n",
    "                                                        epsilon=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"adagrad\":\n",
    "                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,\n",
    "                                                           initial_accumulator_value=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"gd\":\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"momentum\":\n",
    "                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).minimize(\n",
    "                    self.loss)\n",
    "            elif self.optimizer_type == \"yellowfin\":\n",
    "                self.optimizer = YFOptimizer(learning_rate=self.learning_rate, momentum=0.0).minimize(\n",
    "                    self.loss)\n",
    "\n",
    "            # init\n",
    "            self.saver = tf.train.Saver()\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = self._init_session()\n",
    "            self.sess.run(init)\n",
    "\n",
    "            # number of params\n",
    "            total_parameters = 0\n",
    "            for variable in self.weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            if self.verbose > 0:\n",
    "                print(\"#params: %d\" % total_parameters)\n",
    "\n",
    "\n",
    "    def _init_session(self):\n",
    "        config = tf.ConfigProto(device_count={\"gpu\": 0})\n",
    "        config.gpu_options.allow_growth = True\n",
    "        return tf.Session(config=config)\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        weights = dict()\n",
    "\n",
    "        # embeddings\n",
    "        weights[\"feature_embeddings\"] = tf.Variable(\n",
    "            tf.random_normal([self.feature_size, self.embedding_size], 0.0, 0.01),\n",
    "            name=\"feature_embeddings\")  # feature_size * K\n",
    "        weights[\"feature_bias\"] = tf.Variable(\n",
    "            tf.random_uniform([self.feature_size, 1], 0.0, 1.0), name=\"feature_bias\")  # feature_size * 1\n",
    "\n",
    "        # deep layers\n",
    "        num_layer = len(self.deep_layers)\n",
    "        input_size = self.field_size * self.embedding_size\n",
    "        glorot = np.sqrt(2.0 / (input_size + self.deep_layers[0]))\n",
    "        weights[\"layer_0\"] = tf.Variable(\n",
    "            np.random.normal(loc=0, scale=glorot, size=(input_size, self.deep_layers[0])), dtype=np.float32)\n",
    "        weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[0])),\n",
    "                                                        dtype=np.float32)  # 1 * layers[0]\n",
    "        for i in range(1, num_layer):\n",
    "            glorot = np.sqrt(2.0 / (self.deep_layers[i-1] + self.deep_layers[i]))\n",
    "            weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i-1], self.deep_layers[i])),\n",
    "                dtype=np.float32)  # layers[i-1] * layers[i]\n",
    "            weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),\n",
    "                dtype=np.float32)  # 1 * layer[i]\n",
    "\n",
    "        # final concat projection layer\n",
    "        if self.use_fm and self.use_deep:\n",
    "            input_size = self.field_size + self.embedding_size + self.deep_layers[-1]\n",
    "        elif self.use_fm:\n",
    "            input_size = self.field_size + self.embedding_size\n",
    "        elif self.use_deep:\n",
    "            input_size = self.deep_layers[-1]\n",
    "        glorot = np.sqrt(2.0 / (input_size + 1))\n",
    "        weights[\"concat_projection\"] = tf.Variable(\n",
    "                        np.random.normal(loc=0, scale=glorot, size=(input_size, 1)),\n",
    "                        dtype=np.float32)  # layers[i-1]*layers[i]\n",
    "        weights[\"concat_bias\"] = tf.Variable(tf.constant(0.01), dtype=np.float32)\n",
    "\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def batch_norm_layer(self, x, train_phase, scope_bn):\n",
    "        bn_train = batch_norm(x, decay=self.batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                              is_training=True, reuse=None, trainable=True, scope=scope_bn)\n",
    "        bn_inference = batch_norm(x, decay=self.batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                  is_training=False, reuse=True, trainable=True, scope=scope_bn)\n",
    "        z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n",
    "        return z\n",
    "\n",
    "\n",
    "    def get_batch(self, Xi, Xv, y, batch_size, index):\n",
    "        start = index * batch_size\n",
    "        end = (index+1) * batch_size\n",
    "        end = end if end < len(y) else len(y)\n",
    "        return Xi[start:end], Xv[start:end], [[y_] for y_ in y[start:end]]\n",
    "\n",
    "\n",
    "    # shuffle three lists simutaneously\n",
    "    def shuffle_in_unison_scary(self, a, b, c):\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(a)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(b)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(c)\n",
    "\n",
    "\n",
    "    def fit_on_batch(self, Xi, Xv, y):\n",
    "        feed_dict = {self.feat_index: Xi,\n",
    "                     self.feat_value: Xv,\n",
    "                     self.label: y,\n",
    "                     self.dropout_keep_fm: self.dropout_fm,\n",
    "                     self.dropout_keep_deep: self.dropout_deep,\n",
    "                     self.train_phase: True}\n",
    "        import pdb; pdb.set_trace()\n",
    "        loss, opt = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def fit(self, Xi_train, Xv_train, y_train,\n",
    "            Xi_valid=None, Xv_valid=None, y_valid=None,\n",
    "            early_stopping=False, refit=False):\n",
    "        \"\"\"\n",
    "        :param Xi_train: [[ind1_1, ind1_2, ...], [ind2_1, ind2_2, ...], ..., [indi_1, indi_2, ..., indi_j, ...], ...]\n",
    "                         indi_j is the feature index of feature field j of sample i in the training set\n",
    "        :param Xv_train: [[val1_1, val1_2, ...], [val2_1, val2_2, ...], ..., [vali_1, vali_2, ..., vali_j, ...], ...]\n",
    "                         vali_j is the feature value of feature field j of sample i in the training set\n",
    "                         vali_j can be either binary (1/0, for binary/categorical features) or float (e.g., 10.24, for numerical features)\n",
    "        :param y_train: label of each sample in the training set\n",
    "        :param Xi_valid: list of list of feature indices of each sample in the validation set\n",
    "        :param Xv_valid: list of list of feature values of each sample in the validation set\n",
    "        :param y_valid: label of each sample in the validation set\n",
    "        :param early_stopping: perform early stopping or not\n",
    "        :param refit: refit the model on the train+valid dataset or not\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        has_valid = Xv_valid is not None\n",
    "        for epoch in range(self.epoch):\n",
    "            t1 = time.clock()\n",
    "            self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
    "            total_batch = int(len(y_train) / self.batch_size)\n",
    "            for i in range(total_batch):\n",
    "                Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train, self.batch_size, i)\n",
    "                self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "\n",
    "            # evaluate training and validation datasets\n",
    "            train_result = self.evaluate(Xi_train, Xv_train, y_train)\n",
    "            self.train_result.append(train_result)\n",
    "            if has_valid:\n",
    "                valid_result = self.evaluate(Xi_valid, Xv_valid, y_valid)\n",
    "                self.valid_result.append(valid_result)\n",
    "            if self.verbose > 0 and epoch % self.verbose == 0:\n",
    "                if has_valid:\n",
    "                    print(\"[%d] train-result=%.4f, valid-result=%.4f [%.1f s]\"\n",
    "                        % (epoch + 1, train_result, valid_result, time.clock() - t1))\n",
    "                else:\n",
    "                    print(\"[%d] train-result=%.4f [%.1f s]\"\n",
    "                        % (epoch + 1, train_result, time.clock() - t1))\n",
    "            if has_valid and early_stopping and self.training_termination(self.valid_result):\n",
    "                break\n",
    "\n",
    "        # fit a few more epoch on train+valid until result reaches the best_train_score\n",
    "        if has_valid and refit:\n",
    "            if self.greater_is_better:\n",
    "                best_valid_score = max(self.valid_result)\n",
    "            else:\n",
    "                best_valid_score = min(self.valid_result)\n",
    "            best_epoch = self.valid_result.index(best_valid_score)\n",
    "            best_train_score = self.train_result[best_epoch]\n",
    "            Xi_train = Xi_train + Xi_valid\n",
    "            Xv_train = Xv_train + Xv_valid\n",
    "            y_train = y_train + y_valid\n",
    "            for epoch in range(100):\n",
    "                self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
    "                total_batch = int(len(y_train) / self.batch_size)\n",
    "                for i in range(total_batch):\n",
    "                    Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train,\n",
    "                                                                self.batch_size, i)\n",
    "                    self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "                # check\n",
    "                train_result = self.evaluate(Xi_train, Xv_train, y_train)\n",
    "                if abs(train_result - best_train_score) < 0.001 or \\\n",
    "                    (self.greater_is_better and train_result > best_train_score) or \\\n",
    "                    ((not self.greater_is_better) and train_result < best_train_score):\n",
    "                    break\n",
    "\n",
    "\n",
    "    def training_termination(self, valid_result):\n",
    "        if len(valid_result) > 5:\n",
    "            if self.greater_is_better:\n",
    "                if valid_result[-1] < valid_result[-2] and \\\n",
    "                    valid_result[-2] < valid_result[-3] and \\\n",
    "                    valid_result[-3] < valid_result[-4] and \\\n",
    "                    valid_result[-4] < valid_result[-5]:\n",
    "                    return True\n",
    "            else:\n",
    "                if valid_result[-1] > valid_result[-2] and \\\n",
    "                    valid_result[-2] > valid_result[-3] and \\\n",
    "                    valid_result[-3] > valid_result[-4] and \\\n",
    "                    valid_result[-4] > valid_result[-5]:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def predict(self, Xi, Xv):\n",
    "        \"\"\"\n",
    "        :param Xi: list of list of feature indices of each sample in the dataset\n",
    "        :param Xv: list of list of feature values of each sample in the dataset\n",
    "        :return: predicted probability of each sample\n",
    "        \"\"\"\n",
    "        # dummy y\n",
    "        dummy_y = [1] * len(Xi)\n",
    "        batch_index = 0\n",
    "        Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
    "        y_pred = None\n",
    "        while len(Xi_batch) > 0:\n",
    "            num_batch = len(y_batch)\n",
    "            feed_dict = {self.feat_index: Xi_batch,\n",
    "                         self.feat_value: Xv_batch,\n",
    "                         self.label: y_batch,\n",
    "                         self.dropout_keep_fm: [1.0] * len(self.dropout_fm),\n",
    "                         self.dropout_keep_deep: [1.0] * len(self.dropout_deep),\n",
    "                         self.train_phase: False}\n",
    "            batch_out = self.sess.run(self.out, feed_dict=feed_dict)\n",
    "\n",
    "            if batch_index == 0:\n",
    "                y_pred = np.reshape(batch_out, (num_batch,))\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, np.reshape(batch_out, (num_batch,))))\n",
    "\n",
    "            batch_index += 1\n",
    "            Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def evaluate(self, Xi, Xv, y):\n",
    "        \"\"\"\n",
    "        :param Xi: list of list of feature indices of each sample in the dataset\n",
    "        :param Xv: list of list of feature values of each sample in the dataset\n",
    "        :param y: label of each sample in the dataset\n",
    "        :return: metric of the evaluation\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(Xi, Xv)\n",
    "        return self.eval_metric(y, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-29-3cfe2046e1a0>(46)gen_feat_dict()\n",
      "-> df = pd.concat([dfTrain, dfTest]) #首尾相连\n",
      "(Pdb) c\n",
      "func:'gen_feat_dict' took: 5.4813 sec\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dfTrain, dfTest, X_train, y_train, X_test, ids_test, cat_features_indices = _load_data(train.ix[:1000], eval)\n",
    "\n",
    "# folds\n",
    "folds = list(StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True,\n",
    "                             random_state=RANDOM_SEED).split(X_train, y_train))\n",
    "fd = FeatureDictionary(dfTrain=dfTrain, dfTest=dfTest,\n",
    "                           numeric_cols=NUMERIC_COLS,\n",
    "                           ignore_cols=IGNORE_COLS)\n",
    "# data_parser = DataParser(feat_dict=fd)\n",
    "# Xi_train, Xv_train, _ = data_parser.parse(df=dfTrain, has_label=False)\n",
    "# Xi_test, Xv_test, ids_test = data_parser.parse(df=dfTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-29-3cfe2046e1a0>(46)gen_feat_dict()\n",
      "-> df = pd.concat([dfTrain, dfTest]) #首尾相连\n",
      "(Pdb) c\n",
      "func:'gen_feat_dict' took: 3.1272 sec\n",
      "> <ipython-input-29-3cfe2046e1a0>(85)parse()\n",
      "-> for col in dfv.columns:\n",
      "(Pdb) c\n",
      "func:'parse' took: 2.6632 sec\n",
      "> <ipython-input-29-3cfe2046e1a0>(85)parse()\n",
      "-> for col in dfv.columns:\n",
      "(Pdb) c\n",
      "func:'parse' took: 158.6941 sec\n",
      "#params: 787264\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-2da2fe23614a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m }\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0my_train_dfm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_dfm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_base_model_dfm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfTest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfm_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-994be20ad6f0>\u001b[0m in \u001b[0;36m_run_base_model_dfm\u001b[1;34m(dfTrain, dfTest, folds, dfm_params)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mdfm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeepFM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdfm_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mdfm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXi_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXv_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXi_valid_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXv_valid_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;31m#dfm.fit(Xi_train_, Xv_train_, y_train_, Xi_valid_, Xv_valid_, y_valid_,early_stopping=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0my_train_meta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdfm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXi_valid_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXv_valid_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-d119e33cb2b3>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, Xi_train, Xv_train, y_train, Xi_valid, Xv_valid, y_valid, early_stopping, refit)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[1;31m# evaluate training and validation datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m             \u001b[0mtrain_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXi_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXv_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhas_valid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-d119e33cb2b3>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, Xi, Xv, y)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \"\"\"\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-d119e33cb2b3>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, Xi, Xv)\u001b[0m\n\u001b[0;32m    354\u001b[0m                          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_keep_deep\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_deep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m                          self.train_phase: False}\n\u001b[1;32m--> 356\u001b[1;33m             \u001b[0mbatch_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python27\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python27\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mD:\\Python27\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'dict'"
     ]
    }
   ],
   "source": [
    "# ------------------ DeepFM Model ------------------\n",
    "\n",
    "dfm_params = {\n",
    "    \"use_fm\": True,\n",
    "    \"use_deep\": True,\n",
    "    \"embedding_size\": 8,\n",
    "    \"dropout_fm\": [1, 1],\n",
    "    \"deep_layers\": [32],   \n",
    "    \"dropout_deep\": [0.5, 0.5, 0.5],\n",
    "    \"deep_layers_activation\": tf.nn.sigmoid,\n",
    "    \"epoch\": 10,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.004,\n",
    "    \"optimizer_type\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.997,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"verbose\": True,\n",
    "    \"eval_metric\": gini_norm,   \n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "#     \"use_sample_weights\": True,\n",
    "#     \"sample_weights_dict\":{0:1,1:2},\n",
    "    \n",
    "}\n",
    "y_train_dfm, y_test_dfm = _run_base_model_dfm(dfTrain, dfTest, folds, dfm_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拒绝推断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07816186, 0.0199602 , 0.00691541, ..., 0.01492001, 0.03813977,\n",
       "       0.05409675], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_i, y_i = slice(train_data)\n",
    "train_preds = model.run(model.y_prob, X_i, mode='test')\n",
    "train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11544644, 0.11100484, 0.10585465, ..., 0.07886463, 0.39337125,\n",
       "       0.22234131], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_i, y_i = slice(test_data)\n",
    "test_preds = model.run(model.y_prob, X_i, mode='test')\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD3CAYAAADSftWOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAH61JREFUeJzt3Xt05Gd93/H37zczmtF9d6VdaXe9N6/txzb2GiLjyxoS\nuzmADeQGTeHQHE7aNCUtoZDSmJKUtvQQUlJCUpJAcmIIIaeEUy4+dg22oTU2tzU2gzFem33sXa+0\n913dtbrMSHPpHzOjlcRKGo1m5jfPT5/XOT5oJc38vnqwP3r2+3ue5+fl83lERMQtftAFiIjI2im8\nRUQcpPAWEXGQwltExEEKbxERB0XrcZFkMqklLSIiFejr6/Mu9/m6hHexgKq/ZzKZrMn7ukrjsZjG\nYzGNx2IujEcymVz2a2qbiIg4SOEtIuIghbeIiIMU3iIiDlJ4i4g4SOEtIuIghbeIiINCG95zmSzp\nuWzQZYiI1ERow/sjn32K3//kt4MuQ0SkJuq2w7Kestkczx0bYi6TY2QixZaORNAliYhUVShn3meG\nppjL5AA4dmos4GpERKovlDPvB759bP7jRw4NMDyeAuDu2/cGVJGISHWFcuZdCmuAwbHpACsREamN\nUIb3yEQhvKMRn8HRmYCrERGpvlCG9/D4DPFYhJ1bW5mcmWMmnQm6JBGRqiorvI0xtxpjHl/yuXcY\nYw7VpKp1SM9lGZ+cpaszwdbNLQAMjqp1IiLhsmp4G2PuBe4DEgs+9yrgt4DLPuEhSCfPXwRgS2eC\nrZubAbig1omIhEw5M+9jwFtKfzDGdAEfBd5Xq6LWY+DsBABdHQm2biqE99CYwltEwsXL51d/vKQx\nZi/wReAO4CvAB4EZ4IvW2ttWe309n2H5jWfG+P5PJzl4XRub2yJ885kJohGPf3JTBzdf1VavMkRE\nqqJaz7DsA64GPk2hjXK9MebPrbWrzsLr9QzLB5OHgEmuu3oPiaYovSde5sT5i/T07qSv76qq19BI\nXHgmXz1pPBbTeCzmwnis9AzLNYW3tfYp4BVwaTZeTnDX08C5CdqaYySaCj/a1s3NnDh/UUsGRSRU\nQrVUcHJ6luHxxWeZbGqLA3BxejaoskREqq6smbe1th+4bbXPBe304CQAmxeEd2tzDIDJmblAahIR\nqYVQzbynU4XNOImmyPznSuE9pfAWkRAJVXinZgvhHY1c+rHaFN4iEkIhC+/Ck3Ni0Us/VlMsQizq\nq20iIqESyvCORhf/WK3NMc28RSRUQhXe6WLbJBZZEt6JGKnZLHMZPdNSRMIhVOF9ubYJXOp7Lzzn\nW0TEZeEK7+LRr0vDu1XhLSIhE6rwTpd63kvbJsXwHlF4i0hIhCq8V22bTGiLvIiEQ8jCu7jOW20T\nEQm5kIV3cea9TNtE53qLSFiELLwvP/NuiUfx0MxbRMIjZOGdpSnq43uLzy73fY+WRJThCYW3iIRD\nqMI7PZsh3nT5gxJbm2OMjKco58lBIiKNLlThnZrN0hyPXPZrrc0xMtkcE1M611tE3Beu8E5nl515\na5eliIRJqMI7PZtZdJb3QpeWC2rFiYi4LzThnc3lmc3k5p9duZRm3iISJqEJ79KJgvFVZ94KbxFx\nX2jCu7RBR20TEdkIynoAsTHmVuBj1to7jTGvBP4CyAJp4J3W2vM1rLEspQ06y7VN5g+n0lpvEQmB\nVWfexph7gfuA0iPZ/yfwHmvtncBXgQ/UrLo1SK8y845FfJqiPuNaKigiIVBO2+QY8JYFf367tfbH\nxY+jQENMZVPpQngv1/P2PI+OtjgTk+l6liUiUhNeOTsOjTF7gS9aa29b8LmDwGeAn7fWDq70+mQy\nWfNtjcfOpviHbw1x14EOWhOX/52UfGmKoYkMf/i2nbUuR0SkKvr6+rzLfb6snvdSxpi3AX8IvGm1\n4F5QQCWXWlEymZx/39nnzgJDXLl397KtkzPjZzk7eoFX3HjTsr1xly0cD9F4LKXxWMyF8Ugmk8t+\nbc0JZoz5DeBdwJ3W2pF11FVV6fkblpcPboCO1iYAJqZmQxneIrJxrGmpoDEmAnwSaAe+aox53Bjz\n4ZpUtkarLRUE6GgrhvekblqKiNvKmn5aa/uBUr97S82qWYfU/Cad6HyQL9XZGgdgfEo3LUXEbRtm\nkw5AZ3HmPa6Zt4g4LjzhnV55kw5AR3HmPaGZt4g4LjThPb9JZ5nzvGHxDUsREZeFJrxLbZPlNumA\n2iYiEh4hCu9C26R5hbZJZ1vxhqV2WYqI40IU3qvPvFsTMXzfU9tERJwXmvBOz4f38jNv3/foaGnS\nDUsRcV5owjs1m6Ep6hPxL3sMwLyOtib1vEXEeaEK75Vm3SWdrXEmZ+bIZHN1qEpEpDZCFN7ZFZcJ\nlpS2yF9U31tEHBae8E5nV9xdWdJZXOuthzKIiMtCE97pMtsm2mUpImEQivDO5vLMZnLlzby1UUdE\nQiAU4Z1e5eHDC5VOFtTj0ETEZSEJ79VPFCwp3bBUz1tEXBaK8L50HGw5PW8dTiUi7gvFs8BSZTwC\nDeCRQ/1MzcwBYAdGeORQPwB33763dsWJiNRAOGbe6dXPNSlJxAu/r2bSl3/ajoiIC8IR3qWZd3z1\nv0hEfI94LMJM8eENIiIuCkl4l3/DEgoPbCgFvoiIi0IR3ukFDx8uR3NTlFQ6Qz6fr2VZIiI1U1ba\nGWNuBT5mrb3TGHMV8DkgDxwG3m2tDeSUpx8enWRwtp/nXx4G4MWBUVY+U7AgEY+Sy0N6LlvWChUR\nkUaz6szbGHMvcB+QKH7qE8B/sta+FvCAX6ldeeWZyxR+d8Si5f1FoiVRummp1omIuKmcaecx4C3A\nPxT/3Ac8Ufz4YeD1wP2rvUkymaykvlUNnBhgaDgFwMjIIJHs2KqvmU3PAHB84DQTHVGSTcM1qS0I\ntRpnV2k8FtN4LObyeKwa3tbarxhj9i74lGetLTWLLwKd5Vyor69v7dWt4odHn2DP7j2cGT8LpNi5\nfTvbu1tXfd347CBHz5yhY1MXe67YRF/f3qrXFoRkMlmTcXaVxmMxjcdiLozHSr9cKrlhubC/3Q6s\nPtWtsbns2tomzfEYANMptU1ExE2VhPczxpg7ix/fA3yneuVUJlPseUfX2vNOzdWsJhGRWqpkqcX7\ngb81xjQBPwW+XN2S1q70SLNopMzwLm7mmdYNSxFxVFnhba3tB24rfvwi8As1rGnNstlCCz66ysOH\nS5qLM2+1TUTEVaHYpJPJFWbekUh54R2PRfB9T0sFRcRZoQjv0sw74pf343ieR0s8qpm3iDgrJOGd\nw/fAL7NtAoXWyUx6TlvkRcRJoQjvTC5PpMyblSUt8SiZbH5+d6aIiEtCEd7ZbI7IGmbdoJuWIuK2\ncIR3Ll/2MsGSltJGnbTWeouIe0IR3plsruyVJiWXNupo5i0i7glFeGez+bJXmpQ0a6OOiDgsHOGd\nyxGtcOatnreIuMj58M7n82QqmnkXet7aqCMiLnI+vHPFddpr7Xm3zs+8dcNSRNzjfHjPn2uyxtUm\n8aYIvqcbliLiJufDu3Si4FrXeXueR3M8qhuWIuIk58M7myu1Tdb+ozQnYrphKSJOcj+859sma5t5\nQ2mLfI6UZt8i4hjnw7vStglcWi44Npmuak0iIrXmfHivq21S3KgzdlHhLSJucT+8S49Aq2jmXVjr\nParwFhHHOB/emex6bliqbSIibnI+vLNrfATaQqUHEY9NpKpak4hIrVXy9HiMMTHg74G9QBb4bWvt\nkSrWVbZSzzu6xu3xAO0tTQBcGJ2pak0iIrVW6cz7jUDUWnsQ+G/AH1WvpLUp9bwrmXm3txR63udG\npqpak4hIrVUa3i8CUWOMD3QAgR0Qsp6edyTi09Yc4/zIdLXLEhGpqYraJsAkhZbJEaAbePNqL0gm\nkxVeamWDQ8MAjI4MMcDEml/fFM0xNDbDU0//sKK14o2mVuPsKo3HYhqPxVwej0rD+/eAR621HzTG\n7AIeM8bcaK1d9s5fX19fhZda3g+PPkFH5yZght6eHvZs71jze7x0/gQjF0fZte86tne3Vr3Gekom\nkzUZZ1dpPBbTeCzmwnis9Mul0rbJKDBe/HgEiAGRCt9rXUrb4yudNXcUb1qeG1bfW0TcUenM+8+A\nzxpjvgM0AX9grQ0k/UpLBdd6JGxJR1shvNX3FhGXVBTe1tpJ4J9VuZaKXLphub6Zt8JbRFwSnk06\nFazzBuhoVdtERNzjfniv40hYgNbmGNGIr5m3iDjF+fCePxK2wp6353n0bGlWeIuIU5wP70vb4ytf\no92zpZWJqVk9jFhEnOF+eK9je3xJz5YWQDctRcQdzof3/GqTCm9YgsJbRNzjfHhnczl8D/x1tE16\nuwo7KxXeIuIK98M7m6/4ZmVJaeat5YIi4grnwzuTy637QKmeLrVNRMQtzod3NpuveGt8SVtzjJZE\nVOEtIs4IQXjn1rXSBAprvXd0t3JmcGp+9YqISCNzPrwzufy6VpqU7NvRSSab49TgZBWqEhGpLefD\nu9A2Wf9DFPYWzwI/fmbtD3QQEak3p8M7n88X2iZVmnkD9J8ZX+U7RUSC53h4Q5717a4s2bejOPM+\nq5m3iDS+Sh/G0BCKx5qse7XJI4f6gcKqkyP9I/N/vvv2vet6XxGRWnF65l06lKpaDw7u3tTMdCrD\nTDpTlfcTEakVp8O7+ByGde+wLOnqTAAwNDZTlfcTEakVp8M7m1/fgxiW6upsBmBoXOEtIo3N6fCe\nn3lXrW1SmHkPj6Wq8n4iIrVS8Q1LY8wHgV+m8PT4T1lrP1O1qsqUK/W8q9Q26WyLE414mnmLSMOr\nKPWMMXcCB4E7gF8AdlWxprJlS6tNqjTz9j2PLR3NjE6k5x9sLCLSiCqdsr4BeA64H/g/wENVq2gN\nqj3zhkLrJJfPMzqRrtp7iohUW6Vtk25gD/BmYB/woDHmWmttfrkXJJPJCi+1vNLkeGJijIET1elT\n+7lCaB85dopk0r1TBmsxzi7TeCym8VjM5fGoNLyHgSPW2lnAGmNSwFbgwnIv6Ovrq/BSy3vwqW8C\nsLWriz27u6vynvHWKQ4PHCUXaalJzbWUTCadq7mWNB6LaTwWc2E8VvrlUmm/4bvA3cYYzxizA2il\nEOh1dWmdd3V63lDYqON5MKizvUWkgVUU3tbah4BngKco9Lzfba3NVrOwcmRr0POORny2dCQYGp/R\n2d4i0rAqXiporb23moVUojTzrtZqk5Jtm1sYHk9x8sLk/FGxIiKNxO1NOvnqz7wBtm0u7LQ8enKs\nqu8rIlItTod3tso7LEu2bi48kPjoKYW3iDQmp8O7tM57vUfCLtXVmcD3FN4i0rjcDu/iqvJqrjaB\n4k3LzgTHT4/rpqWINCSnw/vSed7V/zG2bW5hNpPjxPmLVX9vEZH1cjq851ebVHnmDbB1k25aikjj\ncjq8szVabQKFmTfAS+p7i0gDcjq8a7XOGwo3LZuiPrZ/tOrvLSKyXo6Hd2nmXf3wjkR8rt27heNn\nx7k4PVv19xcRWQ+nw7t0nnctblgC3LC/m3weDh+r+7EtIiIrcjq8c7k8vgd+DdomADfu7wLg8LGh\nmry/iEilHA/v2tysLLlm92aaoj7PKbxFpME4Hd7ZfL7qW+MXaopFuHbvFvrPTqjvLSINxenwzuWq\nvzV+KfW9RaQROR3e2Vy+JitNSh451M9Mag6Ah777Mo8c6q/ZtURE1sLp8M7larfSpKRnSwsR3+P0\n4GRNryMishZuh3c+X5Ot8QtFIj69Xa0Mj6eYSWdqei0RkXI5G975fJ5sHWbeALt72gE4cU6HVIlI\nY3A2vDPZ2u2uXGpP8VFo/Wcnan4tEZFyOBveqdlCCyMWrf2PsKUjTntLEyfOT5DR+d4i0gCcDe+Z\nVCG8m2KRml/L8zz2bm9ndi7HT/tHan49EZHVrCu8jTHbjDEnjTHXVqugcpVuHjbVYeYNl1onT79w\nvi7XExFZScXJZ4yJAX8DzFSvnPJNp0ptk9rPvAF2bm0jGvF5+oVzdbmeiMhKout47ceBvwY+WM43\nJ5PJdVzqZx09kwJgamqCgRP12bre1e5z6sIk33z8B2xpX8/Q1U61x9l1Go/FNB6LuTweFSWQMeY3\ngUFr7aPGmLLCu6+vr5JLLSsVPQMMsa17C3t2b63qey9nMjPM+R+dYibSTV/f/rpccy2SyWTVx9ll\nGo/FNB6LuTAeK/1yqbRt8i+B1xljHgdeCXzeGNNb4XtVZCZd2LZer7YJqO8tIo2jopm3tfbnSx8X\nA/x3rLV1bQZP1/mGJUBbc4z9V3Ry+NgQ06k5WhKxul1bRGQhd5cKput7w7Lk5ut6yGTzPPvSYF2v\nKyKy0LrD21p7p7X2SDWKWYtL67zr+/vnlusL3SG1TkQkSCGYedf3R7jqik1saovz9E/Pzz8AWUSk\n3kIQ3vVtm/i+x83X9TB2Mc2x02N1vbaISInz4V3vtgnAzdf3AGqdiEhwnA/ves+8AV51zVaiEY8f\nPK/dliISDKfD2/eo6QOIl9OSiHHT1Vt5+fQ4py7ojG8RqT+nw7vWT9FZyV19uwD4VvJUYDWIyMbV\nmAd0lGEmFUx4lx5CPJfJEYv6fP37x+nuTHDPwX11r0VENi6nZ96RAKuPRX32X9HJ5PQcZ4amgitE\nRDYkJ8M7n88H3jYBMLu3AGAHRgOtQ0Q2HifDOz2XJZcn8PDeubWVtuYYR0+NkZ7LBlqLiGwsToZ3\naZlg0OHteR7X7N7MXCbH9549E2gtIrKxuB3eASwTXOr6fYXWycPfPx5wJSKykbgZ3qnSzDvgQoDO\ntjh7ets5MjDKsVPaLi8i9eFmeDdI26Tkhv3dADx8qD/QOkRk43AyvEsPYog0SHjv7m1n25YWHv/R\nKSZn5oIuR0Q2ACfDe75t0gA9bwDf87jn9r2kZ7M89vSJoMsRkQ3AzfBusLYJwOtu2T2/4zKf1znf\nIlJbCu8q6WyL85qbdnB6cIqfvDQUdDkiEnKOh3fAhSzxxjsK55t8TcsGRaTG3A7vBul5l5jdm7ly\nZyc/eP4cQ2MzQZcjIiFW0amCxpgY8FlgLxAHPmKtfbCKda2oEdsmpdMG9/S28/Lpcf7qy89y6yt6\nufv2vUGWJSIhVenM+zeAYWvta4G7gb+sXkmrK602aZSlggtdvWszTTGf518eJpvNBV2OiIRUpeH9\nJeBDxY89IFOdcsoz3YAz75JY1Of6vV3MpDMc0WmDIlIj3nqWtRlj2oEHgb+11n5hue9LJpNVXTv3\nd//3AgMXZnnTqzvxvMYL8NRsjseenSDR5PP+X9seyKPaRCQc+vr6LhsgFT9JxxizC7gf+NRKwb2g\ngEov9TM+/8TjJJqyeJ7Hnt17qva+1XTu4ikOHxtmyu+Zf2RarSWTyaqOs+s0HotpPBZzYTySyeSy\nX6uobWKM6QG+AXzAWvvZCuuq2Ew6Q3O8sZ/g9qprtuF78KX/9yK5nDbtiEh1Vdrz/gNgM/AhY8zj\nxX+aq1jXilwI747WJq7Zs5mT5yf57rOngy5HREKmogS01r4XeG+VaynbTDpDV2ciqMuXre/aHo6e\nHOczDx6m79oeWptjQZckIiHh3CadbC5Pejbb8DNvgE1tcd72umsYmUjz9197IehyRCREnAvvVHGZ\nYEvcjVnsW++6mt297Tx8qJ/nXx4OuhwRCQnnwru0u9KFmTcU1n2/59dfiefBxz7/NIeeOxt0SSIS\nAu6Gd8KN8H7kUD/9Zye448AOxqdm+ejnnuK9n3hcj0wTkXVxIwEXWDjznpsOuJg1uOnqrezqaefx\n5ElePj3O+/7sCczuzbzzTddx4KqtQZcnIo5xb+adcqttstCWjgS/dudVvOmOfdx8XQ8vnhzlQ39z\niEefHAi6NBFxjHMJOL1g5j0RcC2V8DyPvds72Lu9g109bTz8/X7+8ks/5rvPnuaW63v4pdfuD7pE\nEXGAc+E9ky484NfFmfdSO7rbeOtdV/PQ917mxy8OYgdGyeWh79pt9Ha1Eo049xcjEakT5xLw4nQh\nvFubnSv9sja1x/n1X7yGH784yLMvDXLfA4e57wGI+B5XXbGJgwe2c/DADnq7WoMuVUQaiHMJOHC2\n0CzZta2d548EXEyVxGMRbn1FLweu6ubIwAgj4ylGJtK8eHIUe2KUzz30Ar/z1gO88eC+oEsVkQbh\nXHgfPzNOLOqzc1tb0KVUXXM8yquu2Tb/55l0huNnxnny8Dk+/ZWfMDqR5h1vMA15DK6I1JdTTdVM\nNsfAuYvs6W3fEP3g5niU6/d18Za7rqK3q4UvftPyV19+Vk/oERG3wvv0hUnmMjn27egMupS62tQW\n509+97VcuaOTR58c4L9//mnSc9mgyxKRADkV3i+fGQfgyp0bK7wBfvD8OX7x1bvYubWNJw+f43f/\n5DFOD04GXZaIBMSt8D5dCO+NNvMuaYpF+KXX7OPqXZs4NzLNez7+Lf7x0SPzu05FZONw6oblpfDu\nCLiS4EQiPq+7ZTf7d3by1Avn+cI3LPc/cYw7f+4KelvTHMhkiUUjQZcpIjXmTHjn83mOnxlne3cr\nLQk3joOtFc/z2H/FJnb1tPPsS0O8cHyYhw/1A/D5x77GlTs78X2PXC5PNpcnl8vTkijc/Lxhfxc3\n7u+mKaaAF3GZM+E9NJbi4vScDnFaoCkW4dXX99B37TZOnL/I80dPczHlF04s9Dx8z8PzwPc8ZjNZ\nXjg+wpcfe4nmeJRbru/ljpt20HftNgW5iIOcCe/jxZuV+3Zu3JbJcny/cF6KNzfKnt17Lvs9s3NZ\nzg1Pc/LCRY6dGueJZ07xxDOniEV9fs5sKxw34MHUzBzjk2k8PK7oaWN3Tzu7ezvY1dPO1k3N+L7W\nmIs0AmfCe36lyQa9WbleTbEIu3vb2d3bzsEbtzM4OsPRU2McOz3OD54/t+h7C2vo89gTo4s+X5qx\n33R1N9fs2cwV29qJKMxFAlFReBtjfOBTwE1AGvhX1tqj1SxsoVwuz0+PjwAbc5lgtXmex7YtLWzb\n0sLtN25nOp0hl8uTz0O8KUJT1Cefh/GpNCMTKUYn0gyPpzgzNDk/YwdINEXo6mymo7WJlkSUaMQn\nEvGYncuRns2SnsuQms2SzeZpbY7SmojR1dlMb1cLPV2t9G5pYevmZprjUWLRCNGIN797NJ/Pkylu\nRvI9D9/3KtpZms/ntSNVQqnSmfevAglr7e3GmNuAPwV+pXplXXLs1Bif/upPsAOj7NzaypaOxn9q\nvEs8z6P1MjeAPQ82tyfY3J6AnYXP5fN5RibSnBmc5MLoNINjMwyPz3BmcJL8Zd474ntEIz6+73Fu\nOEs2d7nvWnzNWPH7Z+eyLP123wPfL3w94hcC3fc8IhFvPuCz2RxzmRyZbK7wHl84RXM8QnM8xqa2\nON2bmtnUHqcp5tMUjeB5kMsXfrZcvnDBWMQnFo0Qi/o0xXwivs9y+e8BiXiU5nj00q5fr/D50i+N\n0mu94he84uc8vEvf7y3+Okte71G4f7HgJT/z9aXXXliz53mcH5tj4NzEotfOv94rvoNXmCylZ7PM\nZXJQvGdSundS+iXqF19T+HPxa17xaz7zHzfy782pVJbxyXTNr9PW0lSTv6FWGt6vAR4BsNY+aYy5\nuXolXTI6keI/fPI7ZLI5XnPTDn7rl2/QLCpAnufR1Zmgq3PxL9B8Ps9cNkeuuLIlGvGJRn38Jf9f\nZbI5JqfnGJ9KMzE1y8TkLBdnZslm82SzOTK5wv/m84XWTTRSeH0pXAsBy5L/LfyNIZPNkc+A7xda\nRM2JKJm5WRKJBHOZQpCfOH9xvv22YX39fNAVNJav1v6Zsgeu6uaP/s0dVX9fL59feTZ0OcaY+4Cv\nWGsfLv75BHCltfayu0WSyeTaLyIiIvT19V12xlrpzHsCaF/wZ3+54F7p4iIiUplKt8d/D3gjQLHn\n/VzVKhIRkVVVOvO+H3idMeb7FO6P/IvqlSQiIqupqOctIiLBcupUQRERKVB4i4g4SOEtIuKghj/b\nZLWt+MaY3wbeBWSAj1hrHwqk0DoqY0x+D3h78Y9ft9Z+uP5V1k85xzUUv+drwAPW2r+uf5X1U8a/\nH/cA/4XCYoMk8G5rbWhvfpUxHu8H3gHkgI9aa+8PpNA1cmHmPb8VH/iPFLbiA2CM6QX+HXAH8Abg\nj40x8UCqrK+VxuRK4J8DB4HbgNcbYw4EUmX9LDseC3wE2FzXqoKz0r8f7cD/AN5srb0V6Ae6gyiy\njlYaj03Ae4HbgdcDfx5IhRVwIbwXbcUHFm7FvwX4nrU2ba0dB44CYQ8qWHlMTgJ3W2uzxdlUDEjV\nv8S6Wmk8MMb8UwqzqkfqX1ogVhqPgxT2ZfypMeY7wHlr7WD9S6yrlcZjChgAWov/5OpeXYVcCO8O\nYOGBFFljTHSZr10ENsKxg8uOibV2zlo7ZIzxjDEfB56x1r4YSJX1s+x4GGNuoPBX4v8cRGEBWem/\nmW7gLuADwD3A+4wx19S5vnpbaTygMOF5AfgR8Ml6FrYeLoT3Slvxl36tHRirV2EBWvF4AmNMAvhf\nxe/5t3WuLQgrjcc7KZyL+Bjwm8C/N8bcXd/y6m6l8RgGnrbWnrPWTgLfBl5Z7wLrbKXxuAfYDuwD\ndgO/aoy5pc71VcSF8F5pK/5TwGuNMQljTCdwHXC4/iXW3bJjYozxgAeAZ62177LWZoMpsa6WHQ9r\n7b3W2luttXcCnwM+Ya0Ne/tkpf9mfgTcYIzpLs4+b6Mw6wyzlcZjFJgB0tbaFIXJ36a6V1iBht9h\nueBO8QEubcV/I3DUWvtgcbXJv6bwi+ij1tqvBFZsnaw0JkAE+EfgyQUv+aC19lC966yX1f4dWfB9\n/xU4t4FWmyz338zbgd8vfvv/ttZ+LJhK66OM8fgwcDeFfvd3gXtdWH3T8OEtIiI/y4W2iYiILKHw\nFhFxkMJbRMRBCm8REQcpvEVEHKTwFhFxkMJbRMRB/x+6afwVqr7AyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb0eb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "sns.distplot(test_preds, rug=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted overdue rate:0.004527\n",
      "actual overdue rate: 0.065103\n"
     ]
    }
   ],
   "source": [
    "valid_preds = np.array(test_preds)\n",
    "pred_overdue_rate = np.mean(test_preds>=0.4)\n",
    "actual_overdue_rate = test_data[1].mean()\n",
    "print('predicted overdue rate:%f'%pred_overdue_rate)\n",
    "print('actual overdue rate: %f' % actual_overdue_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06510322540998459"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1].mean()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
