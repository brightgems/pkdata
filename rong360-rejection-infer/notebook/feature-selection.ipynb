{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import multiprocess\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.75 s\n"
     ]
    }
   ],
   "source": [
    "%time train = dd.read_parquet('../data/train/part*.parquet', index='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%time df_all = train.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loan_dt', 'label', 'tag', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7',\n",
       "       ...\n",
       "       'f6736', 'f6737', 'f6738', 'f6739', 'f6740', 'f6741', 'f6742', 'f6743',\n",
       "       'f6744', 'f6745'],\n",
       "      dtype='object', length=6748)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 84 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tag</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>33464.000000</td>\n",
       "      <td>99996.000000</td>\n",
       "      <td>99842.000000</td>\n",
       "      <td>22485.000000</td>\n",
       "      <td>99842.000000</td>\n",
       "      <td>64785.000000</td>\n",
       "      <td>99842.000000</td>\n",
       "      <td>99842.000000</td>\n",
       "      <td>99842.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.065683</td>\n",
       "      <td>0.695348</td>\n",
       "      <td>31.386488</td>\n",
       "      <td>4.495842</td>\n",
       "      <td>0.040086</td>\n",
       "      <td>0.018152</td>\n",
       "      <td>178.519058</td>\n",
       "      <td>0.050380</td>\n",
       "      <td>4.711447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.247700</td>\n",
       "      <td>0.460262</td>\n",
       "      <td>69.552246</td>\n",
       "      <td>11.273163</td>\n",
       "      <td>0.252648</td>\n",
       "      <td>0.080223</td>\n",
       "      <td>441.137939</td>\n",
       "      <td>0.295460</td>\n",
       "      <td>43.310730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.502643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.332634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>192.700047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2865.750000</td>\n",
       "      <td>807.000000</td>\n",
       "      <td>11.787868</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26337.482422</td>\n",
       "      <td>12.036999</td>\n",
       "      <td>7082.754883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label           tag            f1            f2            f3  \\\n",
       "count  33464.000000  99996.000000  99842.000000  22485.000000  99842.000000   \n",
       "mean       0.065683      0.695348     31.386488      4.495842      0.040086   \n",
       "std        0.247700      0.460262     69.552246     11.273163      0.252648   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      1.000000      0.000000      1.000000      0.000000   \n",
       "75%        0.000000      1.000000     41.500000      5.000000      0.000000   \n",
       "max        1.000000      1.000000   2865.750000    807.000000     11.787868   \n",
       "\n",
       "                 f4            f5            f6            f7  \n",
       "count  64785.000000  99842.000000  99842.000000  99842.000000  \n",
       "mean       0.018152    178.519058      0.050380      4.711447  \n",
       "std        0.080223    441.137939      0.295460     43.310730  \n",
       "min        0.000000      0.000000      0.000000      0.000000  \n",
       "25%        0.000000     10.502643      0.000000      0.000000  \n",
       "50%        0.000000     69.332634      0.000000      0.000000  \n",
       "75%        0.000000    192.700047      0.000000      0.000000  \n",
       "max        1.000000  26337.482422     12.036999   7082.754883  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_all.iloc[:,:10].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_dt</th>\n",
       "      <th>label</th>\n",
       "      <th>tag</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>...</th>\n",
       "      <th>f6736</th>\n",
       "      <th>f6737</th>\n",
       "      <th>f6738</th>\n",
       "      <th>f6739</th>\n",
       "      <th>f6740</th>\n",
       "      <th>f6741</th>\n",
       "      <th>f6742</th>\n",
       "      <th>f6743</th>\n",
       "      <th>f6744</th>\n",
       "      <th>f6745</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>2018-04-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.341537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35864.0</td>\n",
       "      <td>360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.554676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.084351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>868664.0</td>\n",
       "      <td>360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>72.666664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1198.0</td>\n",
       "      <td>1080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>2018-04-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.044891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82494.0</td>\n",
       "      <td>360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.891247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6748 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           loan_dt  label  tag         f1    f2   f3   f4          f5   f6  \\\n",
       "id                                                                           \n",
       "99996   2018-04-19    NaN    1   0.000000  46.0  0.0  NaN  108.341537  0.0   \n",
       "99997   2018-01-13    NaN    1   0.000000   NaN  0.0  0.0  187.084351  0.0   \n",
       "99998   2018-03-04    NaN    1  72.666664   NaN  0.0  0.0    0.000000  0.0   \n",
       "99999   2018-04-12    NaN    1  29.000000   0.0  0.0  0.0  271.044891  0.0   \n",
       "100000  2018-02-13    NaN    1  65.000000   NaN  0.0  NaN    0.000000  0.0   \n",
       "\n",
       "         f7    ...      f6736     f6737  f6738  f6739  f6740  f6741  f6742  \\\n",
       "id             ...                                                           \n",
       "99996   0.0    ...        0.0   35864.0    360    0.0    NaN    0.0      0   \n",
       "99997   0.0    ...        0.0  868664.0    360    0.0    0.0    0.0      0   \n",
       "99998   0.0    ...        0.0    1198.0   1080    NaN    NaN    0.0      0   \n",
       "99999   0.0    ...        0.0   82494.0    360    0.0    NaN    0.0     16   \n",
       "100000  0.0    ...        0.0       0.0    360    0.0    NaN    0.0      0   \n",
       "\n",
       "        f6743  f6744      f6745  \n",
       "id                               \n",
       "99996     0.0    0.0  17.554676  \n",
       "99997     0.0    0.0   0.000000  \n",
       "99998     0.0    0.0   0.000000  \n",
       "99999     0.0    0.0  23.891247  \n",
       "100000    0.0    0.0   0.000000  \n",
       "\n",
       "[5 rows x 6748 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    69532\n",
       "0    30464\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.loan_dt = df_all.loan_dt.astype(np.datetime64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= df_all[~df_all.label.isnull()]\n",
    "df_eval= df_all[df_all.label.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33464, 6748)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# numpy and pandas for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# model used for feature importances\n",
    "import lightgbm as lgb\n",
    "\n",
    "# utility for early stopping with a validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# memory management\n",
    "import gc\n",
    "\n",
    "# utilities\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "class FeatureSelector():\n",
    "    \"\"\"\n",
    "    Class for performing feature selection for machine learning or data preprocessing.\n",
    "    Implements five different methods to identify features for removal \n",
    "        1. Find columns with a missing percentage greater than a specified threshold\n",
    "        2. Find columns with a single unique value\n",
    "        3. Find collinear variables with a correlation greater than a specified correlation coefficient\n",
    "        4. Find features with 0.0 feature importance from a gradient boosting machine (gbm)\n",
    "        5. Find low importance features that do not contribute to a specified cumulative feature importance from the gbm\n",
    "    Parameters\n",
    "    --------\n",
    "        data : dataframe\n",
    "            A dataset with observations in the rows and features in the columns\n",
    "        labels : array or series, default = None\n",
    "            Array of labels for training the machine learning model to find feature importances. These can be either binary labels\n",
    "            (if task is 'classification') or continuous targets (if task is 'regression').\n",
    "            If no labels are provided, then the feature importance based methods are not available.\n",
    "    Attributes\n",
    "    --------\n",
    "    ops : dict\n",
    "        Dictionary of operations run and features identified for removal\n",
    "    missing_stats : dataframe\n",
    "        The fraction of missing values for all features\n",
    "    record_missing : dataframe\n",
    "        The fraction of missing values for features with missing fraction above threshold\n",
    "    unique_stats : dataframe\n",
    "        Number of unique values for all features\n",
    "    record_single_unique : dataframe\n",
    "        Records the features that have a single unique value\n",
    "    corr_matrix : dataframe\n",
    "        All correlations between all features in the data\n",
    "    record_collinear : dataframe\n",
    "        Records the pairs of collinear variables with a correlation coefficient above the threshold\n",
    "    feature_importances : dataframe\n",
    "        All feature importances from the gradient boosting machine\n",
    "    record_zero_importance : dataframe\n",
    "        Records the zero importance features in the data according to the gbm\n",
    "    record_low_importance : dataframe\n",
    "        Records the lowest importance features not needed to reach the threshold of cumulative importance according to the gbm\n",
    "    Notes\n",
    "    --------\n",
    "        - All 5 operations can be run with the `identify_all` method.\n",
    "        - If using feature importances, one-hot encoding is used for categorical variables which creates new columns\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, labels=None):\n",
    "\n",
    "        # Dataset and optional training labels\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "        if labels is None:\n",
    "            print(\n",
    "                'No labels provided. Feature importance based methods are not available.')\n",
    "\n",
    "        self.base_features = list(data.columns)\n",
    "        self.one_hot_features = None\n",
    "\n",
    "        # Dataframes recording information about features to remove\n",
    "        self.record_missing = None\n",
    "        self.record_single_unique = None\n",
    "        self.record_collinear = None\n",
    "        self.record_zero_importance = None\n",
    "        self.record_low_importance = None\n",
    "\n",
    "        self.missing_stats = None\n",
    "        self.unique_stats = None\n",
    "        self.corr_matrix = None\n",
    "        self.feature_importances = None\n",
    "\n",
    "        # Dictionary to hold removal operations\n",
    "        self.ops = {}\n",
    "\n",
    "        self.one_hot_correlated = False\n",
    "\n",
    "    def identify_missing(self, missing_threshold):\n",
    "        \"\"\"Find the features with a fraction of missing values above `missing_threshold`\"\"\"\n",
    "\n",
    "        self.missing_threshold = missing_threshold\n",
    "\n",
    "        # Calculate the fraction of missing in each column\n",
    "        missing_series = self.data.isnull().sum() / self.data.shape[0]\n",
    "        self.missing_stats = pd.DataFrame(missing_series).rename(\n",
    "            columns={'index': 'feature', 0: 'missing_fraction'})\n",
    "\n",
    "        # Sort with highest number of missing values on top\n",
    "        self.missing_stats = self.missing_stats.sort_values(\n",
    "            'missing_fraction', ascending=False)\n",
    "\n",
    "        # Find the columns with a missing percentage above the threshold\n",
    "        record_missing = pd.DataFrame(missing_series[missing_series > missing_threshold]).reset_index().rename(columns={'index': 'feature',\n",
    "                                                                                                                        0: 'missing_fraction'})\n",
    "\n",
    "        to_drop = list(record_missing['feature'])\n",
    "\n",
    "        self.record_missing = record_missing\n",
    "        self.ops['missing'] = to_drop\n",
    "\n",
    "        print('%d features with greater than %0.2f missing values.\\n' %\n",
    "              (len(self.ops['missing']), self.missing_threshold))\n",
    "\n",
    "    def identify_single_unique(self):\n",
    "        \"\"\"Finds features with only a single unique value. NaNs do not count as a unique value. \"\"\"\n",
    "\n",
    "        # Calculate the unique counts in each column\n",
    "        unique_counts = self.data.nunique()\n",
    "        self.unique_stats = pd.DataFrame(unique_counts).rename(\n",
    "            columns={'index': 'feature', 0: 'nunique'})\n",
    "        self.unique_stats = self.unique_stats.sort_values(\n",
    "            'nunique', ascending=True)\n",
    "\n",
    "        # Find the columns with only one unique count\n",
    "        record_single_unique = pd.DataFrame(unique_counts[unique_counts == 1]).reset_index().rename(columns={'index': 'feature',\n",
    "                                                                                                             0: 'nunique'})\n",
    "\n",
    "        to_drop = list(record_single_unique['feature'])\n",
    "\n",
    "        self.record_single_unique = record_single_unique\n",
    "        self.ops['single_unique'] = to_drop\n",
    "\n",
    "        print('%d features with a single unique value.\\n' %\n",
    "              len(self.ops['single_unique']))\n",
    "\n",
    "    def identify_collinear(self, correlation_threshold, one_hot=False):\n",
    "        \"\"\"\n",
    "        Finds collinear features based on the correlation coefficient between features. \n",
    "        For each pair of features with a correlation coefficient greather than `correlation_threshold`,\n",
    "        only one of the pair is identified for removal. \n",
    "        Using code adapted from: https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "        Parameters\n",
    "        --------\n",
    "        correlation_threshold : float between 0 and 1\n",
    "            Value of the Pearson correlation cofficient for identifying correlation features\n",
    "        one_hot : boolean, default = False\n",
    "            Whether to one-hot encode the features before calculating the correlation coefficients\n",
    "        \"\"\"\n",
    "\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.one_hot_correlated = one_hot\n",
    "\n",
    "        # Calculate the correlations between every column\n",
    "        if one_hot:\n",
    "\n",
    "            # One hot encoding\n",
    "            features = pd.get_dummies(self.data)\n",
    "            self.one_hot_features = [\n",
    "                column for column in features.columns if column not in self.base_features]\n",
    "\n",
    "            # Add one hot encoded data to original data\n",
    "            self.data_all = pd.concat(\n",
    "                [features[self.one_hot_features], self.data], axis=1)\n",
    "\n",
    "            corr_matrix = pd.get_dummies(features).corr()\n",
    "\n",
    "        else:\n",
    "            corr_matrix = self.data.sample(frac=.1).corr()\n",
    "\n",
    "        self.corr_matrix = corr_matrix\n",
    "\n",
    "        # Extract the upper triangle of the correlation matrix\n",
    "        upper = corr_matrix.where(\n",
    "            np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        # Select the features with correlations above the threshold\n",
    "        # Need to use the absolute value\n",
    "        to_drop = [column for column in upper.columns if any(\n",
    "            upper[column].abs() > correlation_threshold)]\n",
    "\n",
    "        # Dataframe to hold correlated pairs\n",
    "        record_collinear = pd.DataFrame(\n",
    "            columns=['drop_feature', 'corr_feature', 'corr_value'])\n",
    "\n",
    "        # Iterate through the columns to drop to record pairs of correlated features\n",
    "        for column in to_drop:\n",
    "\n",
    "            # Find the correlated features\n",
    "            corr_features = list(\n",
    "                upper.index[upper[column].abs() > correlation_threshold])\n",
    "\n",
    "            # Find the correlated values\n",
    "            corr_values = list(\n",
    "                upper[column][upper[column].abs() > correlation_threshold])\n",
    "            drop_features = [column for _ in range(len(corr_features))]\n",
    "\n",
    "            # Record the information (need a temp df for now)\n",
    "            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n",
    "                                              'corr_feature': corr_features,\n",
    "                                              'corr_value': corr_values})\n",
    "\n",
    "            # Add to dataframe\n",
    "            record_collinear = record_collinear.append(\n",
    "                temp_df, ignore_index=True)\n",
    "\n",
    "        self.record_collinear = record_collinear\n",
    "        self.ops['collinear'] = to_drop\n",
    "\n",
    "        print('%d features with a correlation magnitude greater than %0.2f.\\n' % (\n",
    "            len(self.ops['collinear']), self.correlation_threshold))\n",
    "\n",
    "    def identify_zero_importance(self, task, eval_metric=None,\n",
    "                                 n_iterations=5, early_stopping=True):\n",
    "        \"\"\"\n",
    "        Identify the features with zero importance according to a gradient boosting machine.\n",
    "        The gbm can be trained with early stopping using a validation set to prevent overfitting. \n",
    "        The feature importances are averaged over `n_iterations` to reduce variance. \n",
    "        Uses the LightGBM implementation (http://lightgbm.readthedocs.io/en/latest/index.html)\n",
    "        Parameters \n",
    "        --------\n",
    "        eval_metric : string\n",
    "            Evaluation metric to use for the gradient boosting machine for early stopping. Must be\n",
    "            provided if `early_stopping` is True\n",
    "        task : string\n",
    "            The machine learning task, either 'classification' or 'regression'\n",
    "        n_iterations : int, default = 10\n",
    "            Number of iterations to train the gradient boosting machine\n",
    "        early_stopping : boolean, default = True\n",
    "            Whether or not to use early stopping with a validation set when training\n",
    "        Notes\n",
    "        --------\n",
    "        - Features are one-hot encoded to handle the categorical variables before training.\n",
    "        - The gbm is not optimized for any particular task and might need some hyperparameter tuning\n",
    "        - Feature importances, including zero importance features, can change across runs\n",
    "        \"\"\"\n",
    "\n",
    "        if early_stopping and eval_metric is None:\n",
    "            raise ValueError(\"\"\"eval metric must be provided with early stopping. Examples include \"auc\" for classification or\n",
    "                             \"l2\" for regression.\"\"\")\n",
    "\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"No training labels provided.\")\n",
    "\n",
    "        # One hot encoding\n",
    "        features = pd.get_dummies(self.data)\n",
    "        self.one_hot_features = [\n",
    "            column for column in features.columns if column not in self.base_features]\n",
    "\n",
    "        # Add one hot encoded data to original data\n",
    "        self.data_all = pd.concat(\n",
    "            [features[self.one_hot_features], self.data], axis=1)\n",
    "\n",
    "        # Extract feature names\n",
    "        feature_names = list(features.columns)\n",
    "\n",
    "        # Convert to np array\n",
    "        features = np.array(features)\n",
    "        labels = np.array(self.labels).reshape((-1, ))\n",
    "\n",
    "        # Empty array for feature importances\n",
    "        feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "        print('Training Gradient Boosting Model\\n')\n",
    "\n",
    "        # Iterate through each fold\n",
    "        for _ in range(n_iterations):\n",
    "\n",
    "            if task == 'classification':\n",
    "                model = lgb.LGBMClassifier(\n",
    "                    n_estimators=100, learning_rate=0.05, verbose=-1)\n",
    "\n",
    "            elif task == 'regression':\n",
    "                model = lgb.LGBMRegressor(\n",
    "                    n_estimators=100, learning_rate=0.05, verbose=-1)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'Task must be either \"classification\" or \"regression\"')\n",
    "\n",
    "            # If training using early stopping need a validation set\n",
    "            if early_stopping:\n",
    "\n",
    "                train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "                    features, labels, test_size=0.15)\n",
    "\n",
    "                # Train the model with early stopping\n",
    "                model.fit(train_features, train_labels, eval_metric=eval_metric,\n",
    "                          eval_set=[(valid_features, valid_labels)],\n",
    "                          early_stopping_rounds=100, verbose=-1)\n",
    "\n",
    "                # Clean up memory\n",
    "                gc.enable()\n",
    "                del train_features, train_labels, valid_features, valid_labels\n",
    "                gc.collect()\n",
    "\n",
    "            else:\n",
    "                model.fit(features, labels)\n",
    "\n",
    "            # Record the feature importances\n",
    "            feature_importance_values += model.feature_importances_ / n_iterations\n",
    "\n",
    "        feature_importances = pd.DataFrame(\n",
    "            {'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "        # Sort features according to importance\n",
    "        feature_importances = feature_importances.sort_values(\n",
    "            'importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # Normalize the feature importances to add up to one\n",
    "        feature_importances['normalized_importance'] = feature_importances['importance'] / \\\n",
    "            feature_importances['importance'].sum()\n",
    "        feature_importances['cumulative_importance'] = np.cumsum(\n",
    "            feature_importances['normalized_importance'])\n",
    "\n",
    "        # Extract the features with zero importance\n",
    "        record_zero_importance = feature_importances[feature_importances['importance'] == 0.0]\n",
    "\n",
    "        to_drop = list(record_zero_importance['feature'])\n",
    "\n",
    "        self.feature_importances = feature_importances\n",
    "        self.record_zero_importance = record_zero_importance\n",
    "        self.ops['zero_importance'] = to_drop\n",
    "\n",
    "        print('\\n%d features with zero importance after one-hot encoding.\\n' %\n",
    "              len(self.ops['zero_importance']))\n",
    "\n",
    "    def identify_low_importance(self, cumulative_importance):\n",
    "        \"\"\"\n",
    "        Finds the lowest importance features not needed to account for `cumulative_importance` fraction\n",
    "        of the total feature importance from the gradient boosting machine. As an example, if cumulative\n",
    "        importance is set to 0.95, this will retain only the most important features needed to \n",
    "        reach 95% of the total feature importance. The identified features are those not needed.\n",
    "        Parameters\n",
    "        --------\n",
    "        cumulative_importance : float between 0 and 1\n",
    "            The fraction of cumulative importance to account for \n",
    "        \"\"\"\n",
    "\n",
    "        self.cumulative_importance = cumulative_importance\n",
    "\n",
    "        # The feature importances need to be calculated before running\n",
    "        if self.feature_importances is None:\n",
    "            raise NotImplementedError(\"\"\"Feature importances have not yet been determined. \n",
    "                                         Call the `identify_zero_importance` method first.\"\"\")\n",
    "\n",
    "        # Make sure most important features are on top\n",
    "        self.feature_importances = self.feature_importances.sort_values(\n",
    "            'cumulative_importance')\n",
    "\n",
    "        # Identify the features not needed to reach the cumulative_importance\n",
    "        record_low_importance = self.feature_importances[\n",
    "            self.feature_importances['cumulative_importance'] > cumulative_importance]\n",
    "\n",
    "        to_drop = list(record_low_importance['feature'])\n",
    "\n",
    "        self.record_low_importance = record_low_importance\n",
    "        self.ops['low_importance'] = to_drop\n",
    "\n",
    "        print('%d features required for cumulative importance of %0.2f after one hot encoding.' % (len(self.feature_importances) -\n",
    "                                                                                                   len(self.record_low_importance), self.cumulative_importance))\n",
    "        print('%d features do not contribute to cumulative importance of %0.2f.\\n' % (len(self.ops['low_importance']),\n",
    "                                                                                      self.cumulative_importance))\n",
    "\n",
    "    def identify_all(self, selection_params):\n",
    "        \"\"\"\n",
    "        Use all five of the methods to identify features to remove.\n",
    "        Parameters\n",
    "        --------\n",
    "        selection_params : dict\n",
    "           Parameters to use in the five feature selection methhods.\n",
    "           Params must contain the keys ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']\n",
    "        \"\"\"\n",
    "\n",
    "        # Check for all required parameters\n",
    "        for param in ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']:\n",
    "            if param not in selection_params.keys():\n",
    "                raise ValueError(\n",
    "                    '%s is a required parameter for this method.' % param)\n",
    "\n",
    "        # Implement each of the five methods\n",
    "        self.identify_missing(selection_params['missing_threshold'])\n",
    "        self.identify_single_unique()\n",
    "        self.identify_collinear(selection_params['correlation_threshold'])\n",
    "        self.identify_zero_importance(\n",
    "            task=selection_params['task'], eval_metric=selection_params['eval_metric'])\n",
    "        self.identify_low_importance(selection_params['cumulative_importance'])\n",
    "\n",
    "        # Find the number of features identified to drop\n",
    "        self.all_identified = set(list(chain(*list(self.ops.values()))))\n",
    "        self.n_identified = len(self.all_identified)\n",
    "\n",
    "        print('%d total features out of %d identified for removal after one-hot encoding.\\n' % (self.n_identified,\n",
    "                                                                                                self.data_all.shape[1]))\n",
    "\n",
    "    def check_removal(self, keep_one_hot=True):\n",
    "        \"\"\"Check the identified features before removal. Returns a list of the unique features identified.\"\"\"\n",
    "\n",
    "        self.all_identified = set(list(chain(*list(self.ops.values()))))\n",
    "        print('Total of %d features identified for removal' %\n",
    "              len(self.all_identified))\n",
    "\n",
    "        if not keep_one_hot:\n",
    "            if self.one_hot_features is None:\n",
    "                print('Data has not been one-hot encoded')\n",
    "            else:\n",
    "                one_hot_to_remove = [\n",
    "                    x for x in self.one_hot_features if x not in self.all_identified]\n",
    "                print('%d additional one-hot features can be removed' %\n",
    "                      len(one_hot_to_remove))\n",
    "\n",
    "        return list(self.all_identified)\n",
    "\n",
    "    def remove(self, methods, keep_one_hot=True):\n",
    "        \"\"\"\n",
    "        Remove the features from the data according to the specified methods.\n",
    "        Parameters\n",
    "        --------\n",
    "            methods : 'all' or list of methods\n",
    "                If methods == 'all', any methods that have identified features will be used\n",
    "                Otherwise, only the specified methods will be used.\n",
    "                Can be one of ['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance']\n",
    "            keep_one_hot : boolean, default = True\n",
    "                Whether or not to keep one-hot encoded features\n",
    "        Return\n",
    "        --------\n",
    "            data : dataframe\n",
    "                Dataframe with identified features removed\n",
    "        Notes \n",
    "        --------\n",
    "            - If feature importances are used, the one-hot encoded columns will be added to the data (and then may be removed)\n",
    "            - Check the features that will be removed before transforming data!\n",
    "        \"\"\"\n",
    "\n",
    "        features_to_drop = []\n",
    "\n",
    "        if methods == 'all':\n",
    "\n",
    "            # Need to use one-hot encoded data as well\n",
    "            data = self.data_all\n",
    "\n",
    "            print('{} methods have been run\\n'.format(list(self.ops.keys())))\n",
    "\n",
    "            # Find the unique features to drop\n",
    "            features_to_drop = set(list(chain(*list(self.ops.values()))))\n",
    "\n",
    "        else:\n",
    "            # Need to use one-hot encoded data as well\n",
    "            if 'zero_importance' in methods or 'low_importance' in methods or self.one_hot_correlated:\n",
    "                data = self.data_all\n",
    "\n",
    "            else:\n",
    "                data = self.data\n",
    "\n",
    "            # Iterate through the specified methods\n",
    "            for method in methods:\n",
    "\n",
    "                # Check to make sure the method has been run\n",
    "                if method not in self.ops.keys():\n",
    "                    raise NotImplementedError(\n",
    "                        '%s method has not been run' % method)\n",
    "\n",
    "                # Append the features identified for removal\n",
    "                else:\n",
    "                    features_to_drop.append(self.ops[method])\n",
    "\n",
    "            # Find the unique features to drop\n",
    "            features_to_drop = set(list(chain(*features_to_drop)))\n",
    "\n",
    "        features_to_drop = list(features_to_drop)\n",
    "\n",
    "        if not keep_one_hot:\n",
    "\n",
    "            if self.one_hot_features is None:\n",
    "                print('Data has not been one-hot encoded')\n",
    "            else:\n",
    "\n",
    "                features_to_drop = list(\n",
    "                    set(features_to_drop) | set(self.one_hot_features))\n",
    "\n",
    "        # Remove the features and return the data\n",
    "        data = data.drop(columns=features_to_drop)\n",
    "        self.removed_features = features_to_drop\n",
    "\n",
    "        if not keep_one_hot:\n",
    "            print('Removed %d features including one-hot features.' %\n",
    "                  len(features_to_drop))\n",
    "        else:\n",
    "            print('Removed %d features.' % len(features_to_drop))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def plot_missing(self):\n",
    "        \"\"\"Histogram of missing fraction in each feature\"\"\"\n",
    "        if self.record_missing is None:\n",
    "            raise NotImplementedError(\n",
    "                \"Missing values have not been calculated. Run `identify_missing`\")\n",
    "\n",
    "        self.reset_plot()\n",
    "\n",
    "        # Histogram of missing values\n",
    "        plt.style.use('seaborn-white')\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        plt.hist(self.missing_stats['missing_fraction'], bins=np.linspace(\n",
    "            0, 1, 11), edgecolor='k', color='red', linewidth=1.5)\n",
    "        plt.xticks(np.linspace(0, 1, 11))\n",
    "        plt.xlabel('Missing Fraction', size=14)\n",
    "        plt.ylabel('Count of Features', size=14)\n",
    "        plt.title(\"Fraction of Missing Values Histogram\", size=16)\n",
    "\n",
    "    def plot_unique(self):\n",
    "        \"\"\"Histogram of number of unique values in each feature\"\"\"\n",
    "        if self.record_single_unique is None:\n",
    "            raise NotImplementedError(\n",
    "                'Unique values have not been calculated. Run `identify_single_unique`')\n",
    "\n",
    "        self.reset_plot()\n",
    "\n",
    "        # Histogram of number of unique values\n",
    "        self.unique_stats.plot.hist(edgecolor='k', figsize=(7, 5))\n",
    "        plt.ylabel('Frequency', size=14)\n",
    "        plt.xlabel('Unique Values', size=14)\n",
    "        plt.title('Number of Unique Values Histogram', size=16)\n",
    "\n",
    "    def plot_collinear(self, plot_all=False):\n",
    "        \"\"\"\n",
    "        Heatmap of the correlation values. If plot_all = True plots all the correlations otherwise\n",
    "        plots only those features that have a correlation above the threshold\n",
    "        Notes\n",
    "        --------\n",
    "            - Not all of the plotted correlations are above the threshold because this plots\n",
    "            all the variables that have been idenfitied as having even one correlation above the threshold\n",
    "            - The features on the x-axis are those that will be removed. The features on the y-axis\n",
    "            are the correlated features with those on the x-axis\n",
    "        Code adapted from https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "        \"\"\"\n",
    "\n",
    "        if self.record_collinear is None:\n",
    "            raise NotImplementedError(\n",
    "                'Collinear features have not been idenfitied. Run `identify_collinear`.')\n",
    "\n",
    "        if plot_all:\n",
    "            corr_matrix_plot = self.corr_matrix\n",
    "            title = 'All Correlations'\n",
    "\n",
    "        else:\n",
    "            # Identify the correlations that were above the threshold\n",
    "            # columns (x-axis) are features to drop and rows (y_axis) are correlated pairs\n",
    "            corr_matrix_plot = self.corr_matrix.loc[list(set(self.record_collinear['corr_feature'])),\n",
    "                                                    list(set(self.record_collinear['drop_feature']))]\n",
    "\n",
    "            title = \"Correlations Above Threshold\"\n",
    "\n",
    "        f, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "        # Diverging colormap\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "        # Draw the heatmap with a color bar\n",
    "        sns.heatmap(corr_matrix_plot, cmap=cmap, center=0,\n",
    "                    linewidths=.25, cbar_kws={\"shrink\": 0.6})\n",
    "\n",
    "        # Set the ylabels\n",
    "        ax.set_yticks(\n",
    "            [x + 0.5 for x in list(range(corr_matrix_plot.shape[0]))])\n",
    "        ax.set_yticklabels(list(corr_matrix_plot.index),\n",
    "                           size=int(160 / corr_matrix_plot.shape[0]))\n",
    "\n",
    "        # Set the xlabels\n",
    "        ax.set_xticks(\n",
    "            [x + 0.5 for x in list(range(corr_matrix_plot.shape[1]))])\n",
    "        ax.set_xticklabels(list(corr_matrix_plot.columns),\n",
    "                           size=int(160 / corr_matrix_plot.shape[1]))\n",
    "        plt.title(title, size=14)\n",
    "\n",
    "    def plot_feature_importances(self, plot_n=15, threshold=None):\n",
    "        \"\"\"\n",
    "        Plots `plot_n` most important features and the cumulative importance of features.\n",
    "        If `threshold` is provided, prints the number of features needed to reach `threshold` cumulative importance.\n",
    "        Parameters\n",
    "        --------\n",
    "        plot_n : int, default = 15\n",
    "            Number of most important features to plot. Defaults to 15 or the maximum number of features whichever is smaller\n",
    "        threshold : float, between 0 and 1 default = None\n",
    "            Threshold for printing information about cumulative importances\n",
    "        \"\"\"\n",
    "\n",
    "        if self.record_zero_importance is None:\n",
    "            raise NotImplementedError(\n",
    "                'Feature importances have not been determined. Run `idenfity_zero_importance`')\n",
    "\n",
    "        # Need to adjust number of features if greater than the features in the data\n",
    "        if plot_n > self.feature_importances.shape[0]:\n",
    "            plot_n = self.feature_importances.shape[0] - 1\n",
    "\n",
    "        self.reset_plot()\n",
    "\n",
    "        # Make a horizontal bar chart of feature importances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = plt.subplot()\n",
    "\n",
    "        # Need to reverse the index to plot most important on top\n",
    "        # There might be a more efficient method to accomplish this\n",
    "        ax.barh(list(reversed(list(self.feature_importances.index[:plot_n]))),\n",
    "                self.feature_importances['normalized_importance'][:plot_n],\n",
    "                align='center', edgecolor='k')\n",
    "\n",
    "        # Set the yticks and labels\n",
    "        ax.set_yticks(\n",
    "            list(reversed(list(self.feature_importances.index[:plot_n]))))\n",
    "        ax.set_yticklabels(\n",
    "            self.feature_importances['feature'][:plot_n], size=12)\n",
    "\n",
    "        # Plot labeling\n",
    "        plt.xlabel('Normalized Importance', size=16)\n",
    "        plt.title('Feature Importances', size=18)\n",
    "        plt.show()\n",
    "\n",
    "        # Cumulative importance plot\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(list(range(1, len(self.feature_importances) + 1)),\n",
    "                 self.feature_importances['cumulative_importance'], 'r-')\n",
    "        plt.xlabel('Number of Features', size=14)\n",
    "        plt.ylabel('Cumulative Importance', size=14)\n",
    "        plt.title('Cumulative Feature Importance', size=16)\n",
    "\n",
    "        if threshold:\n",
    "\n",
    "            # Index of minimum number of features needed for cumulative importance threshold\n",
    "            # np.where returns the index so need to add 1 to have correct number\n",
    "            importance_index = np.min(\n",
    "                np.where(self.feature_importances['cumulative_importance'] > threshold))\n",
    "            plt.vlines(x=importance_index + 1, ymin=0,\n",
    "                       ymax=1, linestyles='--', colors='blue')\n",
    "            plt.show()\n",
    "\n",
    "            print('%d features required for %0.2f of cumulative importance' %\n",
    "                  (importance_index + 1, threshold))\n",
    "\n",
    "    def reset_plot(self):\n",
    "        plt.rcParams = plt.rcParamsDefault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 features with greater than 0.90 missing values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_data = df_train.drop(['label','tag','loan_dt'],axis=1)\n",
    "y_data = df_train.label\n",
    "fs = FeatureSelector(data=x_data, labels=pd.DataFrame(y_data))\n",
    "%time fs.identify_missing(0.9)\n",
    "fs.identify_single_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosting Model\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.832513\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's auc: 0.822125\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.819788\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.839761\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.821761\n",
      "\n",
      "3734 features with zero importance after one-hot encoding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.identify_zero_importance(eval_metric= 'cross_entropy',task='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.feature_importances.to_csv('../data/feat_importances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2861 features required for cumulative importance of 0.99 after one hot encoding.\n",
      "3884 features do not contribute to cumulative importance of 0.99.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.identify_low_importance(0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove features which have not influence in lgbm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns= fs.check_removal('all')\n",
    "with open('drop_columns.pkl', 'wb') as file:\n",
    "    pickle.dump(drop_columns, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['missing', 'single_unique', 'low_importance', 'zero_importance'] methods have been run\n",
      "\n",
      "Removed 3915 features including one-hot features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f5</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f17</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f28</th>\n",
       "      <th>f35</th>\n",
       "      <th>...</th>\n",
       "      <th>f6711</th>\n",
       "      <th>f6712</th>\n",
       "      <th>f6714</th>\n",
       "      <th>f6723</th>\n",
       "      <th>f6725</th>\n",
       "      <th>f6728</th>\n",
       "      <th>f6734</th>\n",
       "      <th>f6735</th>\n",
       "      <th>f6736</th>\n",
       "      <th>f6745</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.833332</td>\n",
       "      <td>405.743347</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.178469</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>358.594360</td>\n",
       "      <td>146.791306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.073805</td>\n",
       "      <td>0.067</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.422358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>138.166168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.304702</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>356.034424</td>\n",
       "      <td>62.986877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.545374</td>\n",
       "      <td>0.031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.622280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>370.230103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.479102</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>359.424103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.710800</td>\n",
       "      <td>0.037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.780651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132.500000</td>\n",
       "      <td>607.404175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.157072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>665.921265</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.969719</td>\n",
       "      <td>0.023</td>\n",
       "      <td>6454.109863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.698812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>125.333336</td>\n",
       "      <td>85.816986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.489407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>359.755219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.752617</td>\n",
       "      <td>0.158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.836956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2830 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f1          f5  f14    f15  f17  f19       f20  f21       f28  \\\n",
       "id                                                                          \n",
       "1    41.833332  405.743347  NaN  7.000  0.0  7.0  0.003396  9.0  0.178469   \n",
       "2     0.000000  138.166168  NaN  7.000  NaN  1.0  0.016871  NaN  0.304702   \n",
       "3     0.000000  370.230103  NaN  6.600  NaN  0.0  0.007321  NaN  0.479102   \n",
       "4   132.500000  607.404175  NaN    NaN  NaN  0.0  0.006926  NaN  0.157072   \n",
       "5   125.333336   85.816986  NaN  4.875  NaN  0.0  0.010776  NaN  0.489407   \n",
       "\n",
       "    f35    ...           f6711       f6712  f6714  f6723      f6725  f6728  \\\n",
       "id         ...                                                               \n",
       "1   5.0    ...      358.594360  146.791306    0.0    0.0  16.073805  0.067   \n",
       "2   1.0    ...      356.034424   62.986877    0.0    0.0  15.545374  0.031   \n",
       "3   2.0    ...      359.424103    0.000000    6.0    0.0  95.710800  0.037   \n",
       "4   0.0    ...      665.921265   90.000000    0.0    0.0  71.969719  0.023   \n",
       "5   1.0    ...      359.755219    0.000000    0.0    0.0  12.752617  0.158   \n",
       "\n",
       "          f6734     f6735  f6736      f6745  \n",
       "id                                           \n",
       "1      3.260000  0.000000    0.0  20.422358  \n",
       "2           NaN  0.000000    0.0  36.622280  \n",
       "3           NaN  0.000000    0.0  47.780651  \n",
       "4   6454.109863  0.000000    0.0   7.698812  \n",
       "5           NaN  0.002155    0.0  26.836956  \n",
       "\n",
       "[5 rows x 2830 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df = fs.remove(methods='all', keep_one_hot=False)\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = pickle.load(open('drop_columns.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = pickle.load(open('column_types.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_table('../data/valid.txt', dtype= column_types, parse_dates=['loan_dt'])\n",
    "df_valid = df_valid.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_dt</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f6736</th>\n",
       "      <th>f6737</th>\n",
       "      <th>f6738</th>\n",
       "      <th>f6739</th>\n",
       "      <th>f6740</th>\n",
       "      <th>f6741</th>\n",
       "      <th>f6742</th>\n",
       "      <th>f6743</th>\n",
       "      <th>f6744</th>\n",
       "      <th>f6745</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>2018-03-07</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>442.180084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>219.0</td>\n",
       "      <td>20293.0</td>\n",
       "      <td>360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.197765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>2018-03-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6638.0</td>\n",
       "      <td>1080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.197485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32251.0</td>\n",
       "      <td>360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.341463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36614.0</td>\n",
       "      <td>360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.675888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>151.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.253415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6746 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loan_dt    f1    f2   f3   f4          f5   f6   f7   f8  f9  \\\n",
       "id                                                                       \n",
       "100001 2018-03-07  57.0   0.0  0.0  0.0  442.180084  0.0  0.0  NaN   1   \n",
       "100002 2018-03-07   0.0   NaN  0.0  0.0    0.000000  0.0  0.0  NaN   0   \n",
       "100003 2018-01-27   0.0   NaN  0.0  NaN   21.197485  0.0  0.0  NaN   0   \n",
       "100004 2018-03-29   0.0   NaN  0.0  NaN    0.000000  0.0  0.0  0.0   0   \n",
       "100005 2018-01-23   0.0  41.0  0.0  0.0   43.675888  0.0  0.0  NaN   0   \n",
       "\n",
       "          ...      f6736    f6737  f6738  f6739  f6740  f6741  f6742  f6743  \\\n",
       "id        ...                                                                 \n",
       "100001    ...      219.0  20293.0    360    0.0    0.0    0.0      0    0.0   \n",
       "100002    ...        0.0   6638.0   1080    0.0    NaN    0.0      0    0.0   \n",
       "100003    ...        0.0  32251.0    360    NaN    NaN    0.0      0    0.0   \n",
       "100004    ...        0.0  36614.0    360    0.0    NaN    0.0      0    0.0   \n",
       "100005    ...      151.0  16384.0    360    0.0    0.0    0.0      0    0.0   \n",
       "\n",
       "        f6744      f6745  \n",
       "id                        \n",
       "100001    0.0  49.197765  \n",
       "100002    0.0   0.000000  \n",
       "100003    0.0  27.341463  \n",
       "100004    0.0   0.000000  \n",
       "100005    0.0  15.253415  \n",
       "\n",
       "[5 rows x 6746 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fs = df_train.drop(drop_columns, axis=1)\n",
    "eval_fs = df_eval.drop(drop_columns, axis=1)\n",
    "valid_fs = df_valid.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fs.to_pickle('../data/feature-selected/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_fs.to_pickle('../data/feature-selected/eval.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_fs.to_pickle('../data/feature-selected/ccccccc.pkl')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
